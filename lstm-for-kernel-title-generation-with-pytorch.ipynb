{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Organizations.csv', 'Forums.csv', 'Datasources.csv', 'KernelVotes.csv', 'DatasetTags.csv', 'KernelTags.csv', 'KernelVersionOutputFiles.csv', 'CompetitionTags.csv', 'DatasourceObjects.csv', 'UserOrganizations.csv', 'KernelVersionCompetitionSources.csv', 'Tags.csv', 'UserAchievements.csv', 'ForumTopics.csv', 'KernelVersionKernelSources.csv', 'DatasetVersions.csv', 'KernelVersions.csv', 'TeamMemberships.csv', 'ForumMessageVotes.csv', 'Datasets.csv', 'Kernels.csv', 'Competitions.csv', 'Submissions.csv', 'Teams.csv', 'DatasetVotes.csv', 'KernelLanguages.csv', 'DatasourceVersionObjectTables.csv', 'KernelVersionDatasetSources.csv', 'ForumMessages.csv', 'Users.csv', 'UserFollowers.csv']\n"
     ]
    }
   ],
   "source": [
    "# import utilities\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd# data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "\n",
    "# import visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "# import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/Lexie88rus/sequence-models/raw/master/assets/antique-book-book-bindings-2355408.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titles Generation with PyTorch LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first found out about [sequence models](https://medium.com/machine-learning-bites/deeplearning-series-sequence-models-7855babeb586), I was amazed with how easily they can be applied to a wide range of problems: text classification, text generation, music generation, machine translation and others. I shared some great resources on sequence models and LSTM in `\"References and Further Reading\"` section below. You might want to explore them before going through the rest of this kernel, because in this kernel I would like to focus on step-by-step process of creation a model and not on sequence models theory.\n",
    "\n",
    "I got an idea to use [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset to train a model for generation of new kernel titles. This could help to capture some trends for Kaggle kernels and give an inspiration. In this kernel:\n",
    "* I loaded and preprocessed Kaggle data on kernels.\n",
    "* Implemented and trained a sequence model for generation of new Kaggle titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I need to load the data. I will load `Kernels` and `KernelVersions` tables, which contain information on all kernels, total number of votes per kernel (later I will explain why we need this) and kernel titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Load kernel versions\n",
    "kernel_versions = pd.read_csv('../input/KernelVersions.csv')\n",
    "# Load kernels (to retreive TotalVotes)\n",
    "kernels = pd.read_csv('../input/Kernels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a List of Popular Kernel Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to make a list of most popular kernel titles, which will be then converted into word sequences and passed to the model. It comes out that kernel titles are __extremely untidy__: misspelled words, foreign words, special symbols or just have poor names like 'kernel678hggy'.\n",
    "\n",
    "That is why:\n",
    "* __I will drop kernels without votes from the analysis__. I will assume that kernels, which have votes are of better quality and have more meaningful titles.\n",
    "* I will sort kernels by the total number of votes and __take only the most voted ones__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exploring the Dataset',\n",
       " 'Logistic regression analysis',\n",
       " 'Dogs vs. Cats Classification (VGG16 Fine Tuning)',\n",
       " 'Testing',\n",
       " 'Polynomial Regression',\n",
       " 'Cosine Distance',\n",
       " 'reddit play',\n",
       " 'XGBoost on Credit Card Fraud Detection',\n",
       " 'An Interactive Data Science Tutorial e67527',\n",
       " 'PCA_BC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge kernels and versions to retreive kernel title and total votes for kernel\n",
    "kernels_trc = kernels[['CurrentKernelVersionId', 'TotalVotes']].rename(columns={'CurrentKernelVersionId' : 'Id'})\n",
    "kernel_version_trc = kernel_versions[['Id', 'Title']]\n",
    "kernels_titles_votes = kernels_trc.merge(kernel_version_trc)\n",
    "\n",
    "# Sort titles by the number of votes\n",
    "kernels_titles_votes = kernels_titles_votes.sort_values(by=['TotalVotes'])\n",
    "\n",
    "# Retreive the list of popular kernel titles (at leat 1 vote)\n",
    "popular_kernel_titles = kernels_titles_votes[kernels_titles_votes['TotalVotes'] > 0]['Title'].unique().tolist()\n",
    "\n",
    "# Print out some examples\n",
    "popular_kernel_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of kernels is 252866.\n",
      "Total number of kernels with at leat 1 upvote is 50335.\n"
     ]
    }
   ],
   "source": [
    "print('Total number of kernels is {}.'.format(len(kernels)))\n",
    "print('Total number of kernels with at leat 1 upvote is {}.'.format(len(popular_kernel_titles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Kernel Titles and Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try a __word based model__. That's why in the next step I will need to __create a vocabulary__, which then should be used to encode word sequences. \n",
    "\n",
    "To create the vocabulary we have to do the following steps:\n",
    "* Clean each title to remove punctuation and lowercase all the words.\n",
    "* Split each title to words and add each word to the vocabulary.\n",
    "* Introduce a symbol, which denotes the end of the title (I chose `.`, but you can change it) and add it to the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce a function to clean kernel titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, remove punctuation and numbers from kernel titles\n",
    "def clean_title(title):\n",
    "    '''\n",
    "    Function to lowercase, remove punctuation and numbers from kernel titles\n",
    "    '''\n",
    "    # lowercase\n",
    "    title = str(title).lower()\n",
    "    # replace punctuation into spaces\n",
    "    title = re.sub(r\"[,.;@#?!&$%<>-_*/\\()~='+:`]+\\ *\", \" \", title)\n",
    "    title = re.sub('-', ' ', title)\n",
    "    title = re.sub(\"''\", ' ', title)\n",
    "    # replace numbers into spaces\n",
    "    title = re.sub(r\"[0123456789]+\\ *\", \" \", title)\n",
    "    #remove duplicated spaces\n",
    "    title = re.sub(' +', ' ', title)\n",
    "    \n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce a symbol for the end of title and a word extraction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words from kernel titles\n",
    "end_of_sentence = '.' # symbol to denote the end of the sentence\n",
    "def extract_words(title):\n",
    "    '''\n",
    "    Function which transforms kernel title into a list of words ending with 'end_of_sentence' word.\n",
    "    '''\n",
    "    title = clean_title(title)\n",
    "    words = title.split(' ')\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a vocabulary out of a list of titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(titles):\n",
    "    '''\n",
    "    Function to create a vocabulary out of a list of titles\n",
    "    '''\n",
    "    vocab = set()\n",
    "    \n",
    "    for title in titles:\n",
    "        if (clean_title(title) != ''):\n",
    "            words = extract_words(title)\n",
    "            vocab.update(words)\n",
    "        \n",
    "    word_list = list(vocab)\n",
    "    word_list.append(end_of_sentence)\n",
    "    vocabulary = {word_list[n]:n for n in range(0,len(word_list))}\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary out of kernel titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary out of pipular kernel titles\n",
    "vocab = create_vocabulary(popular_kernel_titles)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17711 words in vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} words in vocabulary.'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create a training set for our future model:\n",
    "* __Introduce functions which encode each word into tensor__ using the vocabulary created above. I will use one-hot encoding of words: each word will be represented as a tensor with zeros and ones with all zeros and one in the position which respects to the index of the word in the vocabulary.\n",
    "* __Generate sequences out of kernel titles.__ The length of the sequence is a hyperparameter. I chose sequence length equal to 3. So we will give the model a tensor containing encoding for 3 words and a prediction target, which contains the index of the 4th consequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate word to an index from vocabulary\n",
    "def wordToIndex(word):\n",
    "    if (word != end_of_sentence):\n",
    "        word = clean_title(word)\n",
    "    return vocab[word]\n",
    "\n",
    "# Translate word to 1-hot tensor\n",
    "def wordToTensor(word):\n",
    "    tensor = torch.zeros(1, 1, vocab_size)\n",
    "    tensor[0][0][wordToIndex(word)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a title into a <title_length x 1 x vocab_size>,\n",
    "# or an array of one-hot vectors\n",
    "def titleToTensor(title):\n",
    "    words = extract_words(title)\n",
    "    tensor = torch.zeros(len(words) + 1, 1, vocab_size)\n",
    "    for index in range(len(words)):\n",
    "        tensor[index][0][wordToIndex(words[index])] = 1\n",
    "    \n",
    "    tensor[len(words)][0][vocab[end_of_sentence]] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a sequence of words from title into tensor <sequence_length x 1 x vocab_size>\n",
    "def sequenceToTensor(sequence):\n",
    "    tensor = torch.zeros(len(sequence), 1, vocab_size)\n",
    "    for index in range(len(sequence)):\n",
    "        tensor[index][0][wordToIndex(sequence[index])] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of \"love\" in vocabulary is 15422\n",
      "Index of \"LoVE\" in vocabulary is 15422\n",
      "Tensor representation of \"Fiddling With Python\" is:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 1.]]])\n",
      "Dimensions of title tensor: torch.Size([4, 1, 17711])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate functions above\n",
    "print('Index of \"love\" in vocabulary is {}'.format(wordToIndex('love')))\n",
    "print('Index of \"LoVE\" in vocabulary is {}'.format(wordToIndex('LoVE')))\n",
    "\n",
    "print('Tensor representation of \"Fiddling With Python\" is:')\n",
    "title_tensor = titleToTensor('Fiddling With Python')\n",
    "print(title_tensor)\n",
    "\n",
    "print('Dimensions of title tensor: {}'.format(title_tensor.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences out of titles:\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 3\n",
    "\n",
    "# Generate sequences\n",
    "def generate_sequences(titles):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Loop for all selected titles\n",
    "    for title in titles:\n",
    "        # Run through each title\n",
    "        if clean_title(title) != '' and clean_title(title) != ' ':\n",
    "            words = extract_words(title)\n",
    "            words.append(end_of_sentence)\n",
    "\n",
    "            for i in range(0, len(words) - sequence_length):\n",
    "                sequence = words[i:i + sequence_length]\n",
    "                target = words[i + sequence_length:i + sequence_length + 1]\n",
    "\n",
    "                sequence_tensor = sequenceToTensor(sequence)\n",
    "                target_tensor = sequenceToTensor(target)\n",
    "\n",
    "                sequences.append(sequence_tensor)\n",
    "                targets.append( target_tensor)\n",
    "            \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, targets = generate_sequences(popular_kernel_titles[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is building a simple __LSTM model__ with:\n",
    "* __Input and output sizes__ of the model should be equal to the size of the vocabulary, because we are trying to predict the next word for a sequence;\n",
    "* __LSTM block__ with 128 hidden units;\n",
    "* One __linear layer__ to translate from hidden size into the output size;\n",
    "* __Softmax activation__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM\n",
    "class SimpleLSTM(nn.Module):\n",
    "    '''\n",
    "    Simple LSTM model to generate kernel titles.\n",
    "    Arguments:\n",
    "        - input_size - should be equal to the vocabulary size\n",
    "        - output_size - should be equal to the vocabulary size\n",
    "        - hidden_size - hyperparameter, size of the hidden state of LSTM.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input.view(1, 1, -1), hidden)\n",
    "        \n",
    "        output = self.linear(output[-1].view(1, -1))\n",
    "        \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    # the initialization of the hidden state\n",
    "    # device is cpu or cuda\n",
    "    # I suggest using cude to speedup the computation\n",
    "    def initHidden(self, device):\n",
    "        return (torch.zeros(1, 1, n_hidden).to(device), torch.zeros(1, 1, n_hidden).to(device))\n",
    "\n",
    "# Initialize LSTM\n",
    "n_hidden = 128\n",
    "rnn = SimpleLSTM(vocab_size, n_hidden, vocab_size) # inputs and outputs of RNN are tensors representing words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which converts output into word\n",
    "def wordFromOutput(output):\n",
    "    '''\n",
    "    Functions returns an index from the vocabulary and the corresponding word\n",
    "    '''\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return [k for (k, v) in vocab.items() if v == category_i], category_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model with a sigle forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.7171, -9.7455, -9.8290,  ..., -9.7909, -9.7488, -9.7084]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['occupation'], 13135)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward pass of the model\n",
    "input = titleToTensor('Fiddling with')\n",
    "hidden = (torch.zeros(1, 1, n_hidden), torch.zeros(1, 1, n_hidden))\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "\n",
    "print(output)\n",
    "wordFromOutput(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset and the model are ready and we can start training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce a function, which converts a tensor representing a word into an index from the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert tensor into index in vocabulary\n",
    "def indexFromTensor(target):\n",
    "    '''\n",
    "    Function returns tensor containing target index given tensor representing target word\n",
    "    '''\n",
    "    top_n, top_i = target.topk(1)\n",
    "    target_index = top_i[0].item()\n",
    "    \n",
    "    target_index_tensor = torch.zeros((1), dtype = torch.long)\n",
    "    target_index_tensor[0] = target_index\n",
    "    return target_index_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up learning rate and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 \n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the device for training:\n",
    "\n",
    "_Don't forget to enable GPU in the kernel settings!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device to use\n",
    "# don't forget to turn on GPU on kernel's settings\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training procedure\n",
    "def train(sequence, target, device):\n",
    "    # Move tensors to device\n",
    "    hidden = rnn.initHidden(device)\n",
    "    sequence = sequence.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # Forward step\n",
    "    for i in range(sequence.size()[0]):\n",
    "        output, hidden = rnn(sequence[i], hidden)\n",
    "        \n",
    "    output, hidden = rnn(sequence[i], hidden)\n",
    "    \n",
    "    loss = criterion(output, indexFromTensor(target).to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0% (0m 6s) Loss: 9.8408\n",
      "2000 0% (0m 12s) Loss: 0.8920\n",
      "3000 0% (0m 19s) Loss: 0.8882\n",
      "4000 0% (0m 25s) Loss: 8.8508\n",
      "5000 0% (0m 31s) Loss: 10.4614\n",
      "6000 0% (0m 37s) Loss: 10.1815\n",
      "7000 0% (0m 44s) Loss: 10.3908\n",
      "8000 0% (0m 50s) Loss: 4.4504\n",
      "9000 0% (0m 56s) Loss: 10.6149\n",
      "10000 0% (1m 3s) Loss: 5.5505\n",
      "11000 1% (1m 9s) Loss: 9.0928\n",
      "12000 1% (1m 15s) Loss: 10.2472\n",
      "13000 1% (1m 22s) Loss: 7.7047\n",
      "14000 1% (1m 28s) Loss: 9.8725\n",
      "15000 1% (1m 34s) Loss: 0.8282\n",
      "16000 1% (1m 41s) Loss: 4.8312\n",
      "17000 1% (1m 47s) Loss: 0.6791\n",
      "18000 1% (1m 53s) Loss: 4.2040\n",
      "19000 1% (2m 0s) Loss: 10.7534\n",
      "20000 1% (2m 6s) Loss: 9.1553\n",
      "21000 1% (2m 12s) Loss: 1.1733\n",
      "22000 2% (2m 19s) Loss: 1.0591\n",
      "23000 2% (2m 25s) Loss: 9.8517\n",
      "24000 2% (2m 31s) Loss: 1.3558\n",
      "25000 2% (2m 38s) Loss: 10.2830\n",
      "26000 2% (2m 44s) Loss: 9.7673\n",
      "27000 2% (2m 50s) Loss: 5.4188\n",
      "28000 2% (2m 56s) Loss: 5.5243\n",
      "29000 2% (3m 3s) Loss: 5.1331\n",
      "30000 2% (3m 9s) Loss: 0.8617\n",
      "31000 2% (3m 15s) Loss: 10.3859\n",
      "32000 2% (3m 22s) Loss: 11.2339\n",
      "33000 3% (3m 28s) Loss: 9.4887\n",
      "34000 3% (3m 34s) Loss: 1.3867\n",
      "35000 3% (3m 41s) Loss: 6.6537\n",
      "36000 3% (3m 47s) Loss: 5.8213\n",
      "37000 3% (3m 53s) Loss: 6.6482\n",
      "38000 3% (4m 0s) Loss: 5.8460\n",
      "39000 3% (4m 6s) Loss: 8.6966\n",
      "40000 3% (4m 12s) Loss: 10.5965\n",
      "41000 3% (4m 19s) Loss: 6.9977\n",
      "42000 3% (4m 25s) Loss: 1.6171\n",
      "43000 3% (4m 31s) Loss: 10.4399\n",
      "44000 4% (4m 37s) Loss: 7.6576\n",
      "45000 4% (4m 44s) Loss: 0.6932\n",
      "46000 4% (4m 50s) Loss: 0.9823\n",
      "47000 4% (4m 56s) Loss: 1.1349\n",
      "48000 4% (5m 3s) Loss: 0.5519\n",
      "49000 4% (5m 9s) Loss: 8.0553\n",
      "50000 4% (5m 15s) Loss: 1.5243\n",
      "51000 4% (5m 22s) Loss: 4.6699\n",
      "52000 4% (5m 28s) Loss: 11.0898\n",
      "53000 4% (5m 34s) Loss: 7.7363\n",
      "54000 4% (5m 40s) Loss: 0.9758\n",
      "55000 5% (5m 47s) Loss: 0.7749\n",
      "56000 5% (5m 53s) Loss: 5.6420\n",
      "57000 5% (5m 59s) Loss: 10.7260\n",
      "58000 5% (6m 6s) Loss: 7.0682\n",
      "59000 5% (6m 12s) Loss: 1.1046\n",
      "60000 5% (6m 18s) Loss: 4.7805\n",
      "61000 5% (6m 24s) Loss: 10.2214\n",
      "62000 5% (6m 31s) Loss: 8.2861\n",
      "63000 5% (6m 37s) Loss: 3.9909\n",
      "64000 5% (6m 43s) Loss: 5.5153\n",
      "65000 5% (6m 49s) Loss: 5.1177\n",
      "66000 6% (6m 56s) Loss: 7.8377\n",
      "67000 6% (7m 2s) Loss: 0.6021\n",
      "68000 6% (7m 8s) Loss: 10.5557\n",
      "69000 6% (7m 14s) Loss: 5.4232\n",
      "70000 6% (7m 21s) Loss: 1.2716\n",
      "71000 6% (7m 27s) Loss: 6.7305\n",
      "72000 6% (7m 33s) Loss: 0.7040\n",
      "73000 6% (7m 40s) Loss: 1.3698\n",
      "74000 6% (7m 46s) Loss: 8.6705\n",
      "75000 6% (7m 52s) Loss: 7.8251\n",
      "76000 6% (7m 58s) Loss: 0.8678\n",
      "77000 7% (8m 5s) Loss: 0.6159\n",
      "78000 7% (8m 11s) Loss: 8.7540\n",
      "79000 7% (8m 17s) Loss: 8.7500\n",
      "80000 7% (8m 23s) Loss: 4.7582\n",
      "81000 7% (8m 30s) Loss: 0.7216\n",
      "82000 7% (8m 36s) Loss: 4.2068\n",
      "83000 7% (8m 42s) Loss: 7.7594\n",
      "84000 7% (8m 48s) Loss: 1.0101\n",
      "85000 7% (8m 55s) Loss: 1.3333\n",
      "86000 7% (9m 1s) Loss: 2.1103\n",
      "87000 7% (9m 7s) Loss: 0.8784\n",
      "88000 8% (9m 14s) Loss: 1.5087\n",
      "89000 8% (9m 20s) Loss: 10.2530\n",
      "90000 8% (9m 26s) Loss: 7.2180\n",
      "91000 8% (9m 32s) Loss: 5.1472\n",
      "92000 8% (9m 39s) Loss: 4.5210\n",
      "93000 8% (9m 45s) Loss: 5.3614\n",
      "94000 8% (9m 51s) Loss: 9.8519\n",
      "95000 8% (9m 57s) Loss: 0.4265\n",
      "96000 8% (10m 4s) Loss: 1.1496\n",
      "97000 8% (10m 10s) Loss: 0.6292\n",
      "98000 8% (10m 16s) Loss: 0.9163\n",
      "99000 9% (10m 23s) Loss: 10.3076\n",
      "100000 9% (10m 29s) Loss: 0.6171\n",
      "101000 9% (10m 35s) Loss: 8.4723\n",
      "102000 9% (10m 41s) Loss: 5.5711\n",
      "103000 9% (10m 48s) Loss: 3.8115\n",
      "104000 9% (10m 54s) Loss: 8.4728\n",
      "105000 9% (11m 0s) Loss: 9.5512\n",
      "106000 9% (11m 6s) Loss: 0.7591\n",
      "107000 9% (11m 13s) Loss: 10.1893\n",
      "108000 9% (11m 19s) Loss: 4.2621\n",
      "109000 9% (11m 25s) Loss: 1.1044\n",
      "110000 10% (11m 31s) Loss: 0.9779\n",
      "111000 10% (11m 38s) Loss: 7.6914\n",
      "112000 10% (11m 44s) Loss: 6.4363\n",
      "113000 10% (11m 50s) Loss: 0.5535\n",
      "114000 10% (11m 57s) Loss: 4.3247\n",
      "115000 10% (12m 3s) Loss: 4.6491\n",
      "116000 10% (12m 9s) Loss: 1.3035\n",
      "117000 10% (12m 15s) Loss: 5.6558\n",
      "118000 10% (12m 22s) Loss: 5.5178\n",
      "119000 10% (12m 28s) Loss: 7.9327\n",
      "120000 10% (12m 34s) Loss: 7.1916\n",
      "121000 11% (12m 40s) Loss: 5.2387\n",
      "122000 11% (12m 47s) Loss: 4.6531\n",
      "123000 11% (12m 53s) Loss: 5.2219\n",
      "124000 11% (12m 59s) Loss: 4.8119\n",
      "125000 11% (13m 5s) Loss: 10.0653\n",
      "126000 11% (13m 12s) Loss: 9.4420\n",
      "127000 11% (13m 18s) Loss: 6.0625\n",
      "128000 11% (13m 24s) Loss: 4.5899\n",
      "129000 11% (13m 30s) Loss: 5.7474\n",
      "130000 11% (13m 37s) Loss: 0.7597\n",
      "131000 11% (13m 43s) Loss: 5.3795\n",
      "132000 12% (13m 49s) Loss: 0.6498\n",
      "133000 12% (13m 55s) Loss: 7.5778\n",
      "134000 12% (14m 2s) Loss: 0.4245\n",
      "135000 12% (14m 8s) Loss: 5.4555\n",
      "136000 12% (14m 14s) Loss: 6.6319\n",
      "137000 12% (14m 20s) Loss: 9.4794\n",
      "138000 12% (14m 27s) Loss: 8.5436\n",
      "139000 12% (14m 33s) Loss: 1.4626\n",
      "140000 12% (14m 39s) Loss: 0.5517\n",
      "141000 12% (14m 45s) Loss: 5.9435\n",
      "142000 12% (14m 52s) Loss: 5.4696\n",
      "143000 13% (14m 58s) Loss: 0.4506\n",
      "144000 13% (15m 4s) Loss: 0.9505\n",
      "145000 13% (15m 10s) Loss: 5.8717\n",
      "146000 13% (15m 17s) Loss: 6.4282\n",
      "147000 13% (15m 23s) Loss: 1.1757\n",
      "148000 13% (15m 29s) Loss: 8.6213\n",
      "149000 13% (15m 35s) Loss: 9.0971\n",
      "150000 13% (15m 42s) Loss: 7.9247\n",
      "151000 13% (15m 48s) Loss: 7.3475\n",
      "152000 13% (15m 54s) Loss: 0.8790\n",
      "153000 13% (16m 0s) Loss: 6.6461\n",
      "154000 14% (16m 7s) Loss: 10.0980\n",
      "155000 14% (16m 13s) Loss: 0.5786\n",
      "156000 14% (16m 19s) Loss: 1.0828\n",
      "157000 14% (16m 25s) Loss: 0.8430\n",
      "158000 14% (16m 31s) Loss: 0.5321\n",
      "159000 14% (16m 38s) Loss: 4.0630\n",
      "160000 14% (16m 44s) Loss: 0.4761\n",
      "161000 14% (16m 50s) Loss: 1.1535\n",
      "162000 14% (16m 56s) Loss: 0.5843\n",
      "163000 14% (17m 3s) Loss: 5.1018\n",
      "164000 14% (17m 9s) Loss: 8.6113\n",
      "165000 15% (17m 15s) Loss: 4.2847\n",
      "166000 15% (17m 21s) Loss: 0.6870\n",
      "167000 15% (17m 27s) Loss: 0.3939\n",
      "168000 15% (17m 34s) Loss: 1.2763\n",
      "169000 15% (17m 40s) Loss: 8.2753\n",
      "170000 15% (17m 46s) Loss: 9.3368\n",
      "171000 15% (17m 52s) Loss: 0.6992\n",
      "172000 15% (17m 59s) Loss: 0.3884\n",
      "173000 15% (18m 5s) Loss: 6.6172\n",
      "174000 15% (18m 11s) Loss: 5.4799\n",
      "175000 15% (18m 17s) Loss: 8.5570\n",
      "176000 16% (18m 24s) Loss: 0.3264\n",
      "177000 16% (18m 30s) Loss: 9.1743\n",
      "178000 16% (18m 36s) Loss: 0.3276\n",
      "179000 16% (18m 42s) Loss: 3.2661\n",
      "180000 16% (18m 48s) Loss: 0.9076\n",
      "181000 16% (18m 55s) Loss: 7.8614\n",
      "182000 16% (19m 1s) Loss: 0.4444\n",
      "183000 16% (19m 7s) Loss: 4.9385\n",
      "184000 16% (19m 13s) Loss: 7.3794\n",
      "185000 16% (19m 20s) Loss: 0.5407\n",
      "186000 16% (19m 26s) Loss: 0.6025\n",
      "187000 17% (19m 32s) Loss: 0.8180\n",
      "188000 17% (19m 38s) Loss: 0.8974\n",
      "189000 17% (19m 44s) Loss: 4.8170\n",
      "190000 17% (19m 51s) Loss: 4.4790\n",
      "191000 17% (19m 57s) Loss: 8.2705\n",
      "192000 17% (20m 3s) Loss: 4.7206\n",
      "193000 17% (20m 9s) Loss: 0.3995\n",
      "194000 17% (20m 16s) Loss: 6.1035\n",
      "195000 17% (20m 22s) Loss: 10.6631\n",
      "196000 17% (20m 28s) Loss: 7.7384\n",
      "197000 17% (20m 34s) Loss: 0.7311\n",
      "198000 18% (20m 41s) Loss: 7.8693\n",
      "199000 18% (20m 47s) Loss: 9.3465\n",
      "200000 18% (20m 53s) Loss: 0.4820\n",
      "201000 18% (20m 59s) Loss: 0.7783\n",
      "202000 18% (21m 6s) Loss: 9.7517\n",
      "203000 18% (21m 12s) Loss: 0.8217\n",
      "204000 18% (21m 18s) Loss: 3.9350\n",
      "205000 18% (21m 24s) Loss: 7.9497\n",
      "206000 18% (21m 31s) Loss: 8.4054\n",
      "207000 18% (21m 37s) Loss: 8.1902\n",
      "208000 18% (21m 43s) Loss: 1.2194\n",
      "209000 19% (21m 49s) Loss: 6.9044\n",
      "210000 19% (21m 55s) Loss: 8.0649\n",
      "211000 19% (22m 2s) Loss: 8.5948\n",
      "212000 19% (22m 8s) Loss: 6.0141\n",
      "213000 19% (22m 14s) Loss: 8.2242\n",
      "214000 19% (22m 20s) Loss: 4.9502\n",
      "215000 19% (22m 27s) Loss: 8.6607\n",
      "216000 19% (22m 33s) Loss: 9.6173\n",
      "217000 19% (22m 39s) Loss: 9.3763\n",
      "218000 19% (22m 45s) Loss: 9.0176\n",
      "219000 19% (22m 52s) Loss: 8.3461\n",
      "220000 20% (22m 58s) Loss: 10.2055\n",
      "221000 20% (23m 4s) Loss: 0.7636\n",
      "222000 20% (23m 10s) Loss: 0.7663\n",
      "223000 20% (23m 17s) Loss: 9.1005\n",
      "224000 20% (23m 23s) Loss: 5.3476\n",
      "225000 20% (23m 29s) Loss: 7.3592\n",
      "226000 20% (23m 35s) Loss: 0.4808\n",
      "227000 20% (23m 41s) Loss: 0.4804\n",
      "228000 20% (23m 48s) Loss: 9.1015\n",
      "229000 20% (23m 54s) Loss: 0.6045\n",
      "230000 20% (24m 0s) Loss: 6.6887\n",
      "231000 21% (24m 6s) Loss: 8.8369\n",
      "232000 21% (24m 13s) Loss: 8.0669\n",
      "233000 21% (24m 19s) Loss: 8.2723\n",
      "234000 21% (24m 25s) Loss: 0.5923\n",
      "235000 21% (24m 31s) Loss: 0.2703\n",
      "236000 21% (24m 37s) Loss: 6.2286\n",
      "237000 21% (24m 44s) Loss: 5.3217\n",
      "238000 21% (24m 50s) Loss: 0.2689\n",
      "239000 21% (24m 56s) Loss: 0.3610\n",
      "240000 21% (25m 2s) Loss: 8.0543\n",
      "241000 21% (25m 8s) Loss: 8.0624\n",
      "242000 22% (25m 14s) Loss: 5.8502\n",
      "243000 22% (25m 21s) Loss: 7.1426\n",
      "244000 22% (25m 27s) Loss: 7.1410\n",
      "245000 22% (25m 33s) Loss: 0.4294\n",
      "246000 22% (25m 39s) Loss: 8.9843\n",
      "247000 22% (25m 45s) Loss: 0.3024\n",
      "248000 22% (25m 52s) Loss: 4.4780\n",
      "249000 22% (25m 58s) Loss: 0.3092\n",
      "250000 22% (26m 4s) Loss: 8.0941\n",
      "251000 22% (26m 10s) Loss: 10.1675\n",
      "252000 22% (26m 16s) Loss: 8.0038\n",
      "253000 23% (26m 23s) Loss: 7.0223\n",
      "254000 23% (26m 29s) Loss: 0.6714\n",
      "255000 23% (26m 35s) Loss: 0.8983\n",
      "256000 23% (26m 41s) Loss: 0.3454\n",
      "257000 23% (26m 47s) Loss: 4.1550\n",
      "258000 23% (26m 54s) Loss: 9.0930\n",
      "259000 23% (27m 0s) Loss: 6.5383\n",
      "260000 23% (27m 6s) Loss: 9.0490\n",
      "261000 23% (27m 12s) Loss: 0.8931\n",
      "262000 23% (27m 18s) Loss: 0.9635\n",
      "263000 23% (27m 25s) Loss: 9.4958\n",
      "264000 24% (27m 31s) Loss: 4.5382\n",
      "265000 24% (27m 37s) Loss: 8.8758\n",
      "266000 24% (27m 43s) Loss: 8.3277\n",
      "267000 24% (27m 50s) Loss: 2.8819\n",
      "268000 24% (27m 56s) Loss: 1.1967\n",
      "269000 24% (28m 2s) Loss: 5.5812\n",
      "270000 24% (28m 8s) Loss: 0.5693\n",
      "271000 24% (28m 14s) Loss: 0.4005\n",
      "272000 24% (28m 20s) Loss: 3.6258\n",
      "273000 24% (28m 27s) Loss: 5.8234\n",
      "274000 24% (28m 33s) Loss: 0.4085\n",
      "275000 25% (28m 39s) Loss: 5.8471\n",
      "276000 25% (28m 45s) Loss: 9.0518\n",
      "277000 25% (28m 51s) Loss: 8.0981\n",
      "278000 25% (28m 57s) Loss: 5.8992\n",
      "279000 25% (29m 4s) Loss: 0.8644\n",
      "280000 25% (29m 10s) Loss: 8.5749\n",
      "281000 25% (29m 16s) Loss: 7.4615\n",
      "282000 25% (29m 22s) Loss: 3.7406\n",
      "283000 25% (29m 28s) Loss: 1.5077\n",
      "284000 25% (29m 34s) Loss: 9.4154\n",
      "285000 25% (29m 41s) Loss: 0.7091\n",
      "286000 26% (29m 47s) Loss: 5.9953\n",
      "287000 26% (29m 53s) Loss: 7.0011\n",
      "288000 26% (29m 59s) Loss: 10.3967\n",
      "289000 26% (30m 5s) Loss: 9.5603\n",
      "290000 26% (30m 11s) Loss: 8.7881\n",
      "291000 26% (30m 18s) Loss: 3.2619\n",
      "292000 26% (30m 24s) Loss: 0.7761\n",
      "293000 26% (30m 30s) Loss: 6.9678\n",
      "294000 26% (30m 36s) Loss: 0.4141\n",
      "295000 26% (30m 42s) Loss: 8.8239\n",
      "296000 26% (30m 48s) Loss: 9.2143\n",
      "297000 27% (30m 55s) Loss: 0.1330\n",
      "298000 27% (31m 1s) Loss: 2.7373\n",
      "299000 27% (31m 7s) Loss: 4.6166\n",
      "300000 27% (31m 13s) Loss: 5.4066\n",
      "301000 27% (31m 19s) Loss: 5.2436\n",
      "302000 27% (31m 26s) Loss: 0.1460\n",
      "303000 27% (31m 32s) Loss: 9.1576\n",
      "304000 27% (31m 38s) Loss: 0.5955\n",
      "305000 27% (31m 44s) Loss: 0.1392\n",
      "306000 27% (31m 50s) Loss: 9.6914\n",
      "307000 27% (31m 56s) Loss: 7.6720\n",
      "308000 28% (32m 3s) Loss: 3.6007\n",
      "309000 28% (32m 9s) Loss: 4.0750\n",
      "310000 28% (32m 15s) Loss: 9.5882\n",
      "311000 28% (32m 21s) Loss: 7.7299\n",
      "312000 28% (32m 27s) Loss: 5.0163\n",
      "313000 28% (32m 34s) Loss: 1.6403\n",
      "314000 28% (32m 40s) Loss: 3.8367\n",
      "315000 28% (32m 46s) Loss: 0.5099\n",
      "316000 28% (32m 52s) Loss: 5.0965\n",
      "317000 28% (32m 58s) Loss: 0.1614\n",
      "318000 28% (33m 4s) Loss: 0.9311\n",
      "319000 28% (33m 11s) Loss: 0.6442\n",
      "320000 29% (33m 17s) Loss: 0.6811\n",
      "321000 29% (33m 23s) Loss: 0.2297\n",
      "322000 29% (33m 29s) Loss: 4.2194\n",
      "323000 29% (33m 36s) Loss: 0.2396\n",
      "324000 29% (33m 42s) Loss: 7.9219\n",
      "325000 29% (33m 48s) Loss: 9.1048\n",
      "326000 29% (33m 54s) Loss: 0.3979\n",
      "327000 29% (34m 0s) Loss: 0.6086\n",
      "328000 29% (34m 7s) Loss: 5.1637\n",
      "329000 29% (34m 13s) Loss: 8.2916\n",
      "330000 30% (34m 19s) Loss: 1.5256\n",
      "331000 30% (34m 25s) Loss: 0.3524\n",
      "332000 30% (34m 31s) Loss: 2.8549\n",
      "333000 30% (34m 38s) Loss: 8.3056\n",
      "334000 30% (34m 44s) Loss: 0.5460\n",
      "335000 30% (34m 50s) Loss: 0.5426\n",
      "336000 30% (34m 56s) Loss: 3.7560\n",
      "337000 30% (35m 2s) Loss: 0.5452\n",
      "338000 30% (35m 8s) Loss: 0.6769\n",
      "339000 30% (35m 15s) Loss: 6.2231\n",
      "340000 30% (35m 21s) Loss: 0.5426\n",
      "341000 31% (35m 27s) Loss: 0.3241\n",
      "342000 31% (35m 33s) Loss: 4.0070\n",
      "343000 31% (35m 39s) Loss: 0.0449\n",
      "344000 31% (35m 46s) Loss: 0.1928\n",
      "345000 31% (35m 52s) Loss: 0.2135\n",
      "346000 31% (35m 58s) Loss: 0.2465\n",
      "347000 31% (36m 4s) Loss: 7.7037\n",
      "348000 31% (36m 10s) Loss: 3.7312\n",
      "349000 31% (36m 17s) Loss: 0.5478\n",
      "350000 31% (36m 23s) Loss: 0.2400\n",
      "351000 31% (36m 29s) Loss: 7.4427\n",
      "352000 32% (36m 35s) Loss: 0.1562\n",
      "353000 32% (36m 41s) Loss: 6.3830\n",
      "354000 32% (36m 47s) Loss: 3.9199\n",
      "355000 32% (36m 54s) Loss: 0.4033\n",
      "356000 32% (37m 0s) Loss: 8.7943\n",
      "357000 32% (37m 6s) Loss: 0.1403\n",
      "358000 32% (37m 12s) Loss: 7.7486\n",
      "359000 32% (37m 18s) Loss: 8.1772\n",
      "360000 32% (37m 24s) Loss: 7.3671\n",
      "361000 32% (37m 31s) Loss: 6.7934\n",
      "362000 32% (37m 37s) Loss: 9.9154\n",
      "363000 33% (37m 43s) Loss: 8.6260\n",
      "364000 33% (37m 49s) Loss: 5.7036\n",
      "365000 33% (37m 55s) Loss: 0.3008\n",
      "366000 33% (38m 1s) Loss: 0.3400\n",
      "367000 33% (38m 8s) Loss: 3.1074\n",
      "368000 33% (38m 14s) Loss: 7.8369\n",
      "369000 33% (38m 20s) Loss: 0.4028\n",
      "370000 33% (38m 26s) Loss: 0.2389\n",
      "371000 33% (38m 32s) Loss: 7.0235\n",
      "372000 33% (38m 38s) Loss: 6.5802\n",
      "373000 33% (38m 45s) Loss: 2.8638\n",
      "374000 34% (38m 51s) Loss: 4.7810\n",
      "375000 34% (38m 57s) Loss: 8.7050\n",
      "376000 34% (39m 3s) Loss: 6.1713\n",
      "377000 34% (39m 9s) Loss: 0.3488\n",
      "378000 34% (39m 15s) Loss: 5.7510\n",
      "379000 34% (39m 22s) Loss: 3.7547\n",
      "380000 34% (39m 28s) Loss: 4.2905\n",
      "381000 34% (39m 34s) Loss: 8.0077\n",
      "382000 34% (39m 40s) Loss: 6.1311\n",
      "383000 34% (39m 46s) Loss: 0.4097\n",
      "384000 34% (39m 52s) Loss: 9.4313\n",
      "385000 35% (39m 58s) Loss: 6.0386\n",
      "386000 35% (40m 5s) Loss: 4.7683\n",
      "387000 35% (40m 11s) Loss: 7.5796\n",
      "388000 35% (40m 17s) Loss: 0.6384\n",
      "389000 35% (40m 23s) Loss: 9.0471\n",
      "390000 35% (40m 29s) Loss: 4.0803\n",
      "391000 35% (40m 35s) Loss: 7.0415\n",
      "392000 35% (40m 42s) Loss: 3.7278\n",
      "393000 35% (40m 48s) Loss: 10.7935\n",
      "394000 35% (40m 54s) Loss: 1.1709\n",
      "395000 35% (41m 0s) Loss: 2.9828\n",
      "396000 36% (41m 6s) Loss: 1.8972\n",
      "397000 36% (41m 12s) Loss: 0.0237\n",
      "398000 36% (41m 19s) Loss: 0.3920\n",
      "399000 36% (41m 25s) Loss: 4.5031\n",
      "400000 36% (41m 31s) Loss: 0.1391\n",
      "401000 36% (41m 37s) Loss: 4.3366\n",
      "402000 36% (41m 43s) Loss: 0.0632\n",
      "403000 36% (41m 50s) Loss: 0.4512\n",
      "404000 36% (41m 56s) Loss: 0.7084\n",
      "405000 36% (42m 2s) Loss: 5.9715\n",
      "406000 36% (42m 8s) Loss: 7.5466\n",
      "407000 37% (42m 14s) Loss: 0.6204\n",
      "408000 37% (42m 20s) Loss: 8.3496\n",
      "409000 37% (42m 27s) Loss: 0.0439\n",
      "410000 37% (42m 33s) Loss: 8.6488\n",
      "411000 37% (42m 39s) Loss: 0.7637\n",
      "412000 37% (42m 45s) Loss: 0.5349\n",
      "413000 37% (42m 51s) Loss: 5.3712\n",
      "414000 37% (42m 58s) Loss: 4.7167\n",
      "415000 37% (43m 4s) Loss: 2.4416\n",
      "416000 37% (43m 10s) Loss: 0.1417\n",
      "417000 37% (43m 16s) Loss: 2.9215\n",
      "418000 38% (43m 22s) Loss: 1.8104\n",
      "419000 38% (43m 28s) Loss: 8.1539\n",
      "420000 38% (43m 35s) Loss: 0.0556\n",
      "421000 38% (43m 41s) Loss: 0.9411\n",
      "422000 38% (43m 47s) Loss: 3.7121\n",
      "423000 38% (43m 53s) Loss: 7.1529\n",
      "424000 38% (43m 59s) Loss: 8.8869\n",
      "425000 38% (44m 5s) Loss: 0.2384\n",
      "426000 38% (44m 12s) Loss: 5.2928\n",
      "427000 38% (44m 18s) Loss: 6.5665\n",
      "428000 38% (44m 24s) Loss: 8.1224\n",
      "429000 39% (44m 30s) Loss: 5.0636\n",
      "430000 39% (44m 36s) Loss: 4.1097\n",
      "431000 39% (44m 42s) Loss: 3.2548\n",
      "432000 39% (44m 48s) Loss: 7.1745\n",
      "433000 39% (44m 55s) Loss: 0.4669\n",
      "434000 39% (45m 1s) Loss: 6.0216\n",
      "435000 39% (45m 7s) Loss: 8.6644\n",
      "436000 39% (45m 13s) Loss: 0.8062\n",
      "437000 39% (45m 19s) Loss: 0.5703\n",
      "438000 39% (45m 25s) Loss: 0.1213\n",
      "439000 39% (45m 32s) Loss: 3.7937\n",
      "440000 40% (45m 38s) Loss: 0.1757\n",
      "441000 40% (45m 44s) Loss: 0.0560\n",
      "442000 40% (45m 50s) Loss: 8.6244\n",
      "443000 40% (45m 56s) Loss: 1.1018\n",
      "444000 40% (46m 2s) Loss: 2.0360\n",
      "445000 40% (46m 8s) Loss: 0.0828\n",
      "446000 40% (46m 15s) Loss: 1.2796\n",
      "447000 40% (46m 21s) Loss: 7.9988\n",
      "448000 40% (46m 27s) Loss: 1.9762\n",
      "449000 40% (46m 33s) Loss: 0.1822\n",
      "450000 40% (46m 39s) Loss: 2.9535\n",
      "451000 41% (46m 45s) Loss: 2.3587\n",
      "452000 41% (46m 52s) Loss: 2.5373\n",
      "453000 41% (46m 58s) Loss: 7.8126\n",
      "454000 41% (47m 4s) Loss: 7.2626\n",
      "455000 41% (47m 10s) Loss: 8.4014\n",
      "456000 41% (47m 16s) Loss: 8.2962\n",
      "457000 41% (47m 22s) Loss: 7.6813\n",
      "458000 41% (47m 28s) Loss: 8.3739\n",
      "459000 41% (47m 35s) Loss: 5.1190\n",
      "460000 41% (47m 41s) Loss: 1.2310\n",
      "461000 41% (47m 47s) Loss: 0.9193\n",
      "462000 42% (47m 53s) Loss: 2.3221\n",
      "463000 42% (47m 59s) Loss: 1.8029\n",
      "464000 42% (48m 5s) Loss: 0.0995\n",
      "465000 42% (48m 12s) Loss: 8.4448\n",
      "466000 42% (48m 18s) Loss: 7.9079\n",
      "467000 42% (48m 24s) Loss: 8.2210\n",
      "468000 42% (48m 30s) Loss: 7.6635\n",
      "469000 42% (48m 36s) Loss: 0.0255\n",
      "470000 42% (48m 42s) Loss: 1.4296\n",
      "471000 42% (48m 48s) Loss: 5.8782\n",
      "472000 42% (48m 55s) Loss: 0.1059\n",
      "473000 43% (49m 1s) Loss: 4.5092\n",
      "474000 43% (49m 7s) Loss: 7.9374\n",
      "475000 43% (49m 13s) Loss: 0.0288\n",
      "476000 43% (49m 19s) Loss: 0.4345\n",
      "477000 43% (49m 25s) Loss: 7.2993\n",
      "478000 43% (49m 31s) Loss: 2.1354\n",
      "479000 43% (49m 37s) Loss: 1.9198\n",
      "480000 43% (49m 44s) Loss: 0.0729\n",
      "481000 43% (49m 50s) Loss: 8.3870\n",
      "482000 43% (49m 56s) Loss: 0.1143\n",
      "483000 43% (50m 2s) Loss: 8.4686\n",
      "484000 44% (50m 8s) Loss: 5.8658\n",
      "485000 44% (50m 14s) Loss: 0.0744\n",
      "486000 44% (50m 20s) Loss: 1.2760\n",
      "487000 44% (50m 26s) Loss: 8.2536\n",
      "488000 44% (50m 33s) Loss: 0.0635\n",
      "489000 44% (50m 39s) Loss: 0.0799\n",
      "490000 44% (50m 45s) Loss: 0.1148\n",
      "491000 44% (50m 51s) Loss: 8.5183\n",
      "492000 44% (50m 57s) Loss: 2.7441\n",
      "493000 44% (51m 3s) Loss: 6.1756\n",
      "494000 44% (51m 9s) Loss: 0.5231\n",
      "495000 45% (51m 15s) Loss: 6.9370\n",
      "496000 45% (51m 22s) Loss: 8.1782\n",
      "497000 45% (51m 28s) Loss: 3.0027\n",
      "498000 45% (51m 34s) Loss: 3.1252\n",
      "499000 45% (51m 40s) Loss: 0.1547\n",
      "500000 45% (51m 46s) Loss: 0.1375\n",
      "501000 45% (51m 53s) Loss: 8.5223\n",
      "502000 45% (51m 59s) Loss: 3.9677\n",
      "503000 45% (52m 5s) Loss: 7.3386\n",
      "504000 45% (52m 11s) Loss: 0.3260\n",
      "505000 45% (52m 17s) Loss: 0.2985\n",
      "506000 46% (52m 23s) Loss: 8.0175\n",
      "507000 46% (52m 29s) Loss: 8.6879\n",
      "508000 46% (52m 35s) Loss: 1.2818\n",
      "509000 46% (52m 42s) Loss: 8.4157\n",
      "510000 46% (52m 48s) Loss: 0.2753\n",
      "511000 46% (52m 54s) Loss: 0.0769\n",
      "512000 46% (53m 0s) Loss: 0.3320\n",
      "513000 46% (53m 6s) Loss: 0.0028\n",
      "514000 46% (53m 12s) Loss: 7.1442\n",
      "515000 46% (53m 19s) Loss: 5.7063\n",
      "516000 46% (53m 25s) Loss: 8.0038\n",
      "517000 47% (53m 31s) Loss: 4.7220\n",
      "518000 47% (53m 37s) Loss: 0.1069\n",
      "519000 47% (53m 43s) Loss: 0.2362\n",
      "520000 47% (53m 49s) Loss: 0.0815\n",
      "521000 47% (53m 55s) Loss: 6.7388\n",
      "522000 47% (54m 1s) Loss: 0.0024\n",
      "523000 47% (54m 8s) Loss: 9.2521\n",
      "524000 47% (54m 14s) Loss: 2.4406\n",
      "525000 47% (54m 20s) Loss: 5.8948\n",
      "526000 47% (54m 26s) Loss: 0.0388\n",
      "527000 47% (54m 32s) Loss: 0.0218\n",
      "528000 48% (54m 38s) Loss: 1.2531\n",
      "529000 48% (54m 44s) Loss: 7.7712\n",
      "530000 48% (54m 50s) Loss: 3.1497\n",
      "531000 48% (54m 57s) Loss: 7.3910\n",
      "532000 48% (55m 3s) Loss: 10.3471\n",
      "533000 48% (55m 9s) Loss: 9.8925\n",
      "534000 48% (55m 15s) Loss: 0.0721\n",
      "535000 48% (55m 21s) Loss: 0.0190\n",
      "536000 48% (55m 27s) Loss: 6.9133\n",
      "537000 48% (55m 33s) Loss: 2.4719\n",
      "538000 48% (55m 39s) Loss: 1.8136\n",
      "539000 49% (55m 46s) Loss: 6.5695\n",
      "540000 49% (55m 52s) Loss: 8.2406\n",
      "541000 49% (55m 58s) Loss: 0.1491\n",
      "542000 49% (56m 4s) Loss: 5.3381\n",
      "543000 49% (56m 10s) Loss: 6.7757\n",
      "544000 49% (56m 16s) Loss: 0.1737\n",
      "545000 49% (56m 22s) Loss: 2.2909\n",
      "546000 49% (56m 28s) Loss: 8.1977\n",
      "547000 49% (56m 35s) Loss: 0.3088\n",
      "548000 49% (56m 41s) Loss: 7.9090\n",
      "549000 49% (56m 47s) Loss: 0.8748\n",
      "550000 50% (56m 53s) Loss: 0.0165\n",
      "551000 50% (56m 59s) Loss: 5.2239\n",
      "552000 50% (57m 5s) Loss: 1.2493\n",
      "553000 50% (57m 11s) Loss: 6.8437\n",
      "554000 50% (57m 18s) Loss: 6.9752\n",
      "555000 50% (57m 24s) Loss: 0.0837\n",
      "556000 50% (57m 30s) Loss: 6.3068\n",
      "557000 50% (57m 36s) Loss: 0.1200\n",
      "558000 50% (57m 42s) Loss: 8.5042\n",
      "559000 50% (57m 48s) Loss: 0.8441\n",
      "560000 50% (57m 54s) Loss: 5.1613\n",
      "561000 51% (58m 0s) Loss: 1.9190\n",
      "562000 51% (58m 7s) Loss: 0.1192\n",
      "563000 51% (58m 13s) Loss: 6.1779\n",
      "564000 51% (58m 19s) Loss: 7.9890\n",
      "565000 51% (58m 25s) Loss: 1.3732\n",
      "566000 51% (58m 31s) Loss: 0.2826\n",
      "567000 51% (58m 37s) Loss: 0.1022\n",
      "568000 51% (58m 43s) Loss: 1.9382\n",
      "569000 51% (58m 50s) Loss: 8.2026\n",
      "570000 51% (58m 56s) Loss: 7.2966\n",
      "571000 51% (59m 2s) Loss: 6.5379\n",
      "572000 52% (59m 8s) Loss: 0.0494\n",
      "573000 52% (59m 14s) Loss: 6.3804\n",
      "574000 52% (59m 20s) Loss: 6.9704\n",
      "575000 52% (59m 26s) Loss: 7.2686\n",
      "576000 52% (59m 32s) Loss: 7.7932\n",
      "577000 52% (59m 39s) Loss: 9.3477\n",
      "578000 52% (59m 45s) Loss: 3.1053\n",
      "579000 52% (59m 51s) Loss: 1.3993\n",
      "580000 52% (59m 57s) Loss: 0.0806\n",
      "581000 52% (60m 3s) Loss: 7.9093\n",
      "582000 52% (60m 9s) Loss: 3.5353\n",
      "583000 53% (60m 15s) Loss: 5.3920\n",
      "584000 53% (60m 21s) Loss: 0.0096\n",
      "585000 53% (60m 28s) Loss: 4.9543\n",
      "586000 53% (60m 34s) Loss: 6.5822\n",
      "587000 53% (60m 40s) Loss: 7.1388\n",
      "588000 53% (60m 46s) Loss: 0.0737\n",
      "589000 53% (60m 52s) Loss: 5.8872\n",
      "590000 53% (60m 58s) Loss: 0.0673\n",
      "591000 53% (61m 4s) Loss: 8.7165\n",
      "592000 53% (61m 10s) Loss: 2.6384\n",
      "593000 53% (61m 16s) Loss: 7.2924\n",
      "594000 54% (61m 23s) Loss: 4.2330\n",
      "595000 54% (61m 29s) Loss: 0.0390\n",
      "596000 54% (61m 35s) Loss: 0.1822\n",
      "597000 54% (61m 41s) Loss: 2.1835\n",
      "598000 54% (61m 47s) Loss: 0.7924\n",
      "599000 54% (61m 53s) Loss: 0.4264\n",
      "600000 54% (61m 59s) Loss: 3.1623\n",
      "601000 54% (62m 5s) Loss: 2.5549\n",
      "602000 54% (62m 12s) Loss: 6.9855\n",
      "603000 54% (62m 18s) Loss: 0.0010\n",
      "604000 54% (62m 24s) Loss: 0.0066\n",
      "605000 55% (62m 30s) Loss: 8.2533\n",
      "606000 55% (62m 36s) Loss: 2.8690\n",
      "607000 55% (62m 42s) Loss: 4.5215\n",
      "608000 55% (62m 48s) Loss: 0.0380\n",
      "609000 55% (62m 54s) Loss: 1.2526\n",
      "610000 55% (63m 1s) Loss: 0.1537\n",
      "611000 55% (63m 7s) Loss: 0.0070\n",
      "612000 55% (63m 13s) Loss: 7.5589\n",
      "613000 55% (63m 19s) Loss: 3.6013\n",
      "614000 55% (63m 25s) Loss: 7.1534\n",
      "615000 55% (63m 31s) Loss: 0.0005\n",
      "616000 56% (63m 37s) Loss: 0.6138\n",
      "617000 56% (63m 43s) Loss: 0.1286\n",
      "618000 56% (63m 50s) Loss: 0.1340\n",
      "619000 56% (63m 56s) Loss: 2.3770\n",
      "620000 56% (64m 2s) Loss: 2.7208\n",
      "621000 56% (64m 8s) Loss: 8.8461\n",
      "622000 56% (64m 14s) Loss: 7.6697\n",
      "623000 56% (64m 20s) Loss: 3.1455\n",
      "624000 56% (64m 26s) Loss: 5.7974\n",
      "625000 56% (64m 32s) Loss: 7.2887\n",
      "626000 56% (64m 38s) Loss: 4.9935\n",
      "627000 56% (64m 44s) Loss: 4.3517\n",
      "628000 57% (64m 51s) Loss: 7.2522\n",
      "629000 57% (64m 57s) Loss: 7.6886\n",
      "630000 57% (65m 3s) Loss: 2.2807\n",
      "631000 57% (65m 9s) Loss: 3.9484\n",
      "632000 57% (65m 15s) Loss: 3.0210\n",
      "633000 57% (65m 21s) Loss: 4.6495\n",
      "634000 57% (65m 27s) Loss: 0.5055\n",
      "635000 57% (65m 33s) Loss: 0.0017\n",
      "636000 57% (65m 39s) Loss: 0.7371\n",
      "637000 57% (65m 45s) Loss: 0.0634\n",
      "638000 57% (65m 51s) Loss: 1.0147\n",
      "639000 58% (65m 57s) Loss: 2.0150\n",
      "640000 58% (66m 4s) Loss: 0.0399\n",
      "641000 58% (66m 10s) Loss: 0.5287\n",
      "642000 58% (66m 16s) Loss: 1.8665\n",
      "643000 58% (66m 22s) Loss: 1.4961\n",
      "644000 58% (66m 28s) Loss: 0.0237\n",
      "645000 58% (66m 34s) Loss: 0.0924\n",
      "646000 58% (66m 40s) Loss: 0.1164\n",
      "647000 58% (66m 46s) Loss: 7.9855\n",
      "648000 58% (66m 52s) Loss: 1.6161\n",
      "649000 59% (66m 58s) Loss: 4.8645\n",
      "650000 59% (67m 5s) Loss: 0.0726\n",
      "651000 59% (67m 11s) Loss: 3.1786\n",
      "652000 59% (67m 17s) Loss: 0.0055\n",
      "653000 59% (67m 23s) Loss: 0.1383\n",
      "654000 59% (67m 29s) Loss: 7.0446\n",
      "655000 59% (67m 35s) Loss: 2.1964\n",
      "656000 59% (67m 41s) Loss: 0.7031\n",
      "657000 59% (67m 48s) Loss: 0.0491\n",
      "658000 59% (67m 54s) Loss: 3.4244\n",
      "659000 59% (68m 0s) Loss: 7.8462\n",
      "660000 60% (68m 6s) Loss: 0.0815\n",
      "661000 60% (68m 12s) Loss: 0.0114\n",
      "662000 60% (68m 18s) Loss: 5.4348\n",
      "663000 60% (68m 24s) Loss: 1.7045\n",
      "664000 60% (68m 30s) Loss: 5.7142\n",
      "665000 60% (68m 37s) Loss: 0.5680\n",
      "666000 60% (68m 43s) Loss: 0.7727\n",
      "667000 60% (68m 49s) Loss: 7.9319\n",
      "668000 60% (68m 55s) Loss: 7.2208\n",
      "669000 60% (69m 1s) Loss: 1.7521\n",
      "670000 60% (69m 7s) Loss: 0.0294\n",
      "671000 61% (69m 13s) Loss: 3.2487\n",
      "672000 61% (69m 19s) Loss: 5.1852\n",
      "673000 61% (69m 25s) Loss: 0.4338\n",
      "674000 61% (69m 31s) Loss: 0.0553\n",
      "675000 61% (69m 38s) Loss: 0.0373\n",
      "676000 61% (69m 44s) Loss: 0.0396\n",
      "677000 61% (69m 50s) Loss: 0.1350\n",
      "678000 61% (69m 56s) Loss: 5.3833\n",
      "679000 61% (70m 2s) Loss: 5.8751\n",
      "680000 61% (70m 8s) Loss: 0.0219\n",
      "681000 61% (70m 14s) Loss: 0.0751\n",
      "682000 62% (70m 21s) Loss: 0.0013\n",
      "683000 62% (70m 27s) Loss: 0.8937\n",
      "684000 62% (70m 33s) Loss: 0.1218\n",
      "685000 62% (70m 39s) Loss: 0.6434\n",
      "686000 62% (70m 46s) Loss: 0.4778\n",
      "687000 62% (70m 52s) Loss: 0.0016\n",
      "688000 62% (70m 58s) Loss: 1.2848\n",
      "689000 62% (71m 4s) Loss: 6.7996\n",
      "690000 62% (71m 11s) Loss: 3.7494\n",
      "691000 62% (71m 17s) Loss: 0.0020\n",
      "692000 62% (71m 23s) Loss: 0.0249\n",
      "693000 63% (71m 29s) Loss: 6.0652\n",
      "694000 63% (71m 36s) Loss: 0.0010\n",
      "695000 63% (71m 42s) Loss: 0.0123\n",
      "696000 63% (71m 48s) Loss: 0.0230\n",
      "697000 63% (71m 55s) Loss: 3.6184\n",
      "698000 63% (72m 1s) Loss: 0.0114\n",
      "699000 63% (72m 7s) Loss: 5.4570\n",
      "700000 63% (72m 13s) Loss: 8.6603\n",
      "701000 63% (72m 20s) Loss: 0.1239\n",
      "702000 63% (72m 26s) Loss: 0.1026\n",
      "703000 63% (72m 32s) Loss: 4.1750\n",
      "704000 64% (72m 38s) Loss: 4.8379\n",
      "705000 64% (72m 45s) Loss: 0.3373\n",
      "706000 64% (72m 51s) Loss: 5.8485\n",
      "707000 64% (72m 57s) Loss: 6.3559\n",
      "708000 64% (73m 3s) Loss: 0.0046\n",
      "709000 64% (73m 9s) Loss: 0.2178\n",
      "710000 64% (73m 16s) Loss: 0.1288\n",
      "711000 64% (73m 22s) Loss: 6.1183\n",
      "712000 64% (73m 28s) Loss: 3.9232\n",
      "713000 64% (73m 35s) Loss: 0.0136\n",
      "714000 64% (73m 41s) Loss: 6.2477\n",
      "715000 65% (73m 47s) Loss: 6.3052\n",
      "716000 65% (73m 53s) Loss: 0.2921\n",
      "717000 65% (73m 59s) Loss: 0.0108\n",
      "718000 65% (74m 6s) Loss: 7.6593\n",
      "719000 65% (74m 12s) Loss: 4.7804\n",
      "720000 65% (74m 18s) Loss: 0.0113\n",
      "721000 65% (74m 24s) Loss: 1.8434\n",
      "722000 65% (74m 31s) Loss: 0.0057\n",
      "723000 65% (74m 37s) Loss: 6.0026\n",
      "724000 65% (74m 43s) Loss: 0.0793\n",
      "725000 65% (74m 49s) Loss: 0.2929\n",
      "726000 66% (74m 56s) Loss: 3.8506\n",
      "727000 66% (75m 2s) Loss: 2.4613\n",
      "728000 66% (75m 8s) Loss: 4.5551\n",
      "729000 66% (75m 14s) Loss: 3.6804\n",
      "730000 66% (75m 21s) Loss: 4.0850\n",
      "731000 66% (75m 27s) Loss: 5.7021\n",
      "732000 66% (75m 33s) Loss: 0.0428\n",
      "733000 66% (75m 39s) Loss: 0.0028\n",
      "734000 66% (75m 46s) Loss: 0.0624\n",
      "735000 66% (75m 52s) Loss: 1.2974\n",
      "736000 66% (75m 58s) Loss: 0.6049\n",
      "737000 67% (76m 4s) Loss: 0.0129\n",
      "738000 67% (76m 10s) Loss: 0.3803\n",
      "739000 67% (76m 17s) Loss: 0.0457\n",
      "740000 67% (76m 23s) Loss: 0.1250\n",
      "741000 67% (76m 29s) Loss: 0.0231\n",
      "742000 67% (76m 35s) Loss: 4.9274\n",
      "743000 67% (76m 42s) Loss: 0.4400\n",
      "744000 67% (76m 48s) Loss: 0.0465\n",
      "745000 67% (76m 54s) Loss: 5.1344\n",
      "746000 67% (77m 0s) Loss: 0.0030\n",
      "747000 67% (77m 7s) Loss: 4.8866\n",
      "748000 68% (77m 13s) Loss: 5.0187\n",
      "749000 68% (77m 19s) Loss: 5.5005\n",
      "750000 68% (77m 25s) Loss: 0.0567\n",
      "751000 68% (77m 32s) Loss: 5.6838\n",
      "752000 68% (77m 38s) Loss: 0.0219\n",
      "753000 68% (77m 44s) Loss: 3.8697\n",
      "754000 68% (77m 51s) Loss: 4.4124\n",
      "755000 68% (77m 57s) Loss: 0.2532\n",
      "756000 68% (78m 3s) Loss: 0.0097\n",
      "757000 68% (78m 9s) Loss: 0.7879\n",
      "758000 68% (78m 16s) Loss: 5.4477\n",
      "759000 69% (78m 22s) Loss: 0.0272\n",
      "760000 69% (78m 28s) Loss: 7.6718\n",
      "761000 69% (78m 35s) Loss: 0.0293\n",
      "762000 69% (78m 41s) Loss: 0.0672\n",
      "763000 69% (78m 47s) Loss: 1.3250\n",
      "764000 69% (78m 53s) Loss: 3.2418\n",
      "765000 69% (79m 0s) Loss: 6.0864\n",
      "766000 69% (79m 6s) Loss: 2.8663\n",
      "767000 69% (79m 12s) Loss: 9.1977\n",
      "768000 69% (79m 18s) Loss: 0.2916\n",
      "769000 69% (79m 25s) Loss: 0.0273\n",
      "770000 70% (79m 31s) Loss: 6.1167\n",
      "771000 70% (79m 37s) Loss: 4.7306\n",
      "772000 70% (79m 44s) Loss: 1.0550\n",
      "773000 70% (79m 50s) Loss: 0.0004\n",
      "774000 70% (79m 56s) Loss: 2.2528\n",
      "775000 70% (80m 2s) Loss: 5.9989\n",
      "776000 70% (80m 9s) Loss: 3.3130\n",
      "777000 70% (80m 15s) Loss: 0.0044\n",
      "778000 70% (80m 21s) Loss: 5.4305\n",
      "779000 70% (80m 27s) Loss: 0.0054\n",
      "780000 70% (80m 34s) Loss: 0.0094\n",
      "781000 71% (80m 40s) Loss: 0.9606\n",
      "782000 71% (80m 46s) Loss: 5.1239\n",
      "783000 71% (80m 52s) Loss: 0.5683\n",
      "784000 71% (80m 59s) Loss: 0.0002\n",
      "785000 71% (81m 5s) Loss: 5.9164\n",
      "786000 71% (81m 11s) Loss: 5.9197\n",
      "787000 71% (81m 17s) Loss: 1.3235\n",
      "788000 71% (81m 24s) Loss: 4.5361\n",
      "789000 71% (81m 30s) Loss: 5.0115\n",
      "790000 71% (81m 36s) Loss: 4.9193\n",
      "791000 71% (81m 43s) Loss: 0.2347\n",
      "792000 72% (81m 49s) Loss: 0.1035\n",
      "793000 72% (81m 55s) Loss: 0.1951\n",
      "794000 72% (82m 1s) Loss: 5.3982\n",
      "795000 72% (82m 8s) Loss: 0.0117\n",
      "796000 72% (82m 14s) Loss: 0.1335\n",
      "797000 72% (82m 20s) Loss: 0.1423\n",
      "798000 72% (82m 26s) Loss: 1.5799\n",
      "799000 72% (82m 33s) Loss: 8.0099\n",
      "800000 72% (82m 39s) Loss: 4.7656\n",
      "801000 72% (82m 45s) Loss: 0.0319\n",
      "802000 72% (82m 52s) Loss: 0.0876\n",
      "803000 73% (82m 58s) Loss: 4.1122\n",
      "804000 73% (83m 4s) Loss: 0.1386\n",
      "805000 73% (83m 11s) Loss: 0.0061\n",
      "806000 73% (83m 17s) Loss: 0.1653\n",
      "807000 73% (83m 23s) Loss: 2.3902\n",
      "808000 73% (83m 30s) Loss: 0.0615\n",
      "809000 73% (83m 36s) Loss: 3.3347\n",
      "810000 73% (83m 42s) Loss: 5.3754\n",
      "811000 73% (83m 48s) Loss: 0.0012\n",
      "812000 73% (83m 55s) Loss: 0.2714\n",
      "813000 73% (84m 1s) Loss: 2.4643\n",
      "814000 74% (84m 7s) Loss: 1.2114\n",
      "815000 74% (84m 14s) Loss: 5.4643\n",
      "816000 74% (84m 20s) Loss: 2.9393\n",
      "817000 74% (84m 26s) Loss: 0.0006\n",
      "818000 74% (84m 32s) Loss: 0.7329\n",
      "819000 74% (84m 39s) Loss: 0.0001\n",
      "820000 74% (84m 45s) Loss: 5.8215\n",
      "821000 74% (84m 51s) Loss: 4.4814\n",
      "822000 74% (84m 58s) Loss: 5.2498\n",
      "823000 74% (85m 4s) Loss: 2.8658\n",
      "824000 74% (85m 10s) Loss: 0.0534\n",
      "825000 75% (85m 16s) Loss: 1.2617\n",
      "826000 75% (85m 23s) Loss: 0.0095\n",
      "827000 75% (85m 29s) Loss: 1.7798\n",
      "828000 75% (85m 35s) Loss: 0.4200\n",
      "829000 75% (85m 42s) Loss: 0.7739\n",
      "830000 75% (85m 48s) Loss: 0.0409\n",
      "831000 75% (85m 54s) Loss: 0.6079\n",
      "832000 75% (86m 0s) Loss: 4.1148\n",
      "833000 75% (86m 7s) Loss: 0.0016\n",
      "834000 75% (86m 13s) Loss: 0.0203\n",
      "835000 75% (86m 19s) Loss: 6.4116\n",
      "836000 76% (86m 26s) Loss: 0.0361\n",
      "837000 76% (86m 32s) Loss: 0.0084\n",
      "838000 76% (86m 38s) Loss: 0.0052\n",
      "839000 76% (86m 44s) Loss: 5.2414\n",
      "840000 76% (86m 51s) Loss: 0.0006\n",
      "841000 76% (86m 57s) Loss: 0.0497\n",
      "842000 76% (87m 3s) Loss: 0.0248\n",
      "843000 76% (87m 10s) Loss: 0.0322\n",
      "844000 76% (87m 16s) Loss: 0.0354\n",
      "845000 76% (87m 22s) Loss: 0.1008\n",
      "846000 76% (87m 29s) Loss: 0.0030\n",
      "847000 77% (87m 35s) Loss: 3.3111\n",
      "848000 77% (87m 41s) Loss: 0.4605\n",
      "849000 77% (87m 47s) Loss: 4.9317\n",
      "850000 77% (87m 54s) Loss: 4.6193\n",
      "851000 77% (88m 0s) Loss: 1.7447\n",
      "852000 77% (88m 6s) Loss: 4.7475\n",
      "853000 77% (88m 13s) Loss: 0.0018\n",
      "854000 77% (88m 19s) Loss: 4.7719\n",
      "855000 77% (88m 25s) Loss: 1.9074\n",
      "856000 77% (88m 31s) Loss: 0.0207\n",
      "857000 77% (88m 38s) Loss: 2.6107\n",
      "858000 78% (88m 44s) Loss: 0.2060\n",
      "859000 78% (88m 50s) Loss: 4.0267\n",
      "860000 78% (88m 56s) Loss: 0.0201\n",
      "861000 78% (89m 2s) Loss: 0.1063\n",
      "862000 78% (89m 8s) Loss: 0.2688\n",
      "863000 78% (89m 15s) Loss: 3.2268\n",
      "864000 78% (89m 21s) Loss: 2.7671\n",
      "865000 78% (89m 27s) Loss: 3.2348\n",
      "866000 78% (89m 33s) Loss: 0.0135\n",
      "867000 78% (89m 39s) Loss: 2.0016\n",
      "868000 78% (89m 45s) Loss: 0.0022\n",
      "869000 79% (89m 51s) Loss: 2.8091\n",
      "870000 79% (89m 58s) Loss: 0.2371\n",
      "871000 79% (90m 4s) Loss: 0.1944\n",
      "872000 79% (90m 10s) Loss: 0.0792\n",
      "873000 79% (90m 16s) Loss: 3.0383\n",
      "874000 79% (90m 22s) Loss: 0.0063\n",
      "875000 79% (90m 28s) Loss: 2.1925\n",
      "876000 79% (90m 34s) Loss: 0.3356\n",
      "877000 79% (90m 40s) Loss: 4.6249\n",
      "878000 79% (90m 46s) Loss: 3.3643\n",
      "879000 79% (90m 52s) Loss: 2.1320\n",
      "880000 80% (90m 58s) Loss: 4.7535\n",
      "881000 80% (91m 5s) Loss: 0.1170\n",
      "882000 80% (91m 11s) Loss: 4.3468\n",
      "883000 80% (91m 17s) Loss: 4.8476\n",
      "884000 80% (91m 23s) Loss: 0.1116\n",
      "885000 80% (91m 29s) Loss: 1.5620\n",
      "886000 80% (91m 35s) Loss: 2.2542\n",
      "887000 80% (91m 41s) Loss: 0.3780\n",
      "888000 80% (91m 47s) Loss: 0.0411\n",
      "889000 80% (91m 53s) Loss: 4.8429\n",
      "890000 80% (92m 0s) Loss: 0.0023\n",
      "891000 81% (92m 6s) Loss: 1.5892\n",
      "892000 81% (92m 12s) Loss: 0.0313\n",
      "893000 81% (92m 18s) Loss: 0.0918\n",
      "894000 81% (92m 24s) Loss: 2.3502\n",
      "895000 81% (92m 30s) Loss: 1.7890\n",
      "896000 81% (92m 36s) Loss: 2.3196\n",
      "897000 81% (92m 42s) Loss: 1.5760\n",
      "898000 81% (92m 48s) Loss: 5.2245\n",
      "899000 81% (92m 54s) Loss: 4.1536\n",
      "900000 81% (93m 0s) Loss: 1.1939\n",
      "901000 81% (93m 7s) Loss: 4.7812\n",
      "902000 82% (93m 13s) Loss: 3.5886\n",
      "903000 82% (93m 19s) Loss: 3.5233\n",
      "904000 82% (93m 25s) Loss: 0.3901\n",
      "905000 82% (93m 31s) Loss: 0.0003\n",
      "906000 82% (93m 37s) Loss: 4.2900\n",
      "907000 82% (93m 44s) Loss: 0.0006\n",
      "908000 82% (93m 50s) Loss: 0.7999\n",
      "909000 82% (93m 56s) Loss: 0.4274\n",
      "910000 82% (94m 2s) Loss: 2.7036\n",
      "911000 82% (94m 8s) Loss: 1.7115\n",
      "912000 82% (94m 14s) Loss: 0.0046\n",
      "913000 83% (94m 20s) Loss: 4.6626\n",
      "914000 83% (94m 26s) Loss: 4.4290\n",
      "915000 83% (94m 32s) Loss: 0.2680\n",
      "916000 83% (94m 38s) Loss: 3.0535\n",
      "917000 83% (94m 44s) Loss: 0.0002\n",
      "918000 83% (94m 50s) Loss: 0.2489\n",
      "919000 83% (94m 57s) Loss: 3.4130\n",
      "920000 83% (95m 3s) Loss: 0.0167\n",
      "921000 83% (95m 9s) Loss: 4.0532\n",
      "922000 83% (95m 15s) Loss: 4.3506\n",
      "923000 83% (95m 21s) Loss: 0.8051\n",
      "924000 84% (95m 27s) Loss: 0.0358\n",
      "925000 84% (95m 33s) Loss: 0.0759\n",
      "926000 84% (95m 39s) Loss: 5.0300\n",
      "927000 84% (95m 45s) Loss: 2.3036\n",
      "928000 84% (95m 51s) Loss: 3.6136\n",
      "929000 84% (95m 57s) Loss: 0.0062\n",
      "930000 84% (96m 4s) Loss: 2.8010\n",
      "931000 84% (96m 10s) Loss: 0.0051\n",
      "932000 84% (96m 16s) Loss: 0.0024\n",
      "933000 84% (96m 22s) Loss: 1.4508\n",
      "934000 84% (96m 28s) Loss: 0.0036\n",
      "935000 85% (96m 34s) Loss: 3.5190\n",
      "936000 85% (96m 40s) Loss: 0.1795\n",
      "937000 85% (96m 46s) Loss: 0.0490\n",
      "938000 85% (96m 52s) Loss: 1.7975\n",
      "939000 85% (96m 58s) Loss: 0.0008\n",
      "940000 85% (97m 5s) Loss: 0.3064\n",
      "941000 85% (97m 11s) Loss: 3.5296\n",
      "942000 85% (97m 17s) Loss: 0.6703\n",
      "943000 85% (97m 23s) Loss: 0.4493\n",
      "944000 85% (97m 29s) Loss: 0.0374\n",
      "945000 85% (97m 35s) Loss: 3.3658\n",
      "946000 86% (97m 41s) Loss: 0.0000\n",
      "947000 86% (97m 47s) Loss: 0.0004\n",
      "948000 86% (97m 53s) Loss: 0.0362\n",
      "949000 86% (97m 59s) Loss: 0.2281\n",
      "950000 86% (98m 5s) Loss: 2.0232\n",
      "951000 86% (98m 11s) Loss: 0.4682\n",
      "952000 86% (98m 18s) Loss: 0.0071\n",
      "953000 86% (98m 24s) Loss: 3.3460\n",
      "954000 86% (98m 30s) Loss: 0.4056\n",
      "955000 86% (98m 36s) Loss: 0.0108\n",
      "956000 86% (98m 42s) Loss: 2.9435\n",
      "957000 87% (98m 48s) Loss: 0.0042\n",
      "958000 87% (98m 54s) Loss: 0.0001\n",
      "959000 87% (99m 0s) Loss: 0.0382\n",
      "960000 87% (99m 7s) Loss: 0.0770\n",
      "961000 87% (99m 13s) Loss: 1.8730\n",
      "962000 87% (99m 19s) Loss: 0.0002\n",
      "963000 87% (99m 25s) Loss: 0.0493\n",
      "964000 87% (99m 31s) Loss: 1.8660\n",
      "965000 87% (99m 37s) Loss: 3.7774\n",
      "966000 87% (99m 43s) Loss: 0.5623\n",
      "967000 87% (99m 49s) Loss: 1.0333\n",
      "968000 88% (99m 55s) Loss: 0.6790\n",
      "969000 88% (100m 2s) Loss: 4.5842\n",
      "970000 88% (100m 8s) Loss: 0.0238\n",
      "971000 88% (100m 14s) Loss: 0.5991\n",
      "972000 88% (100m 20s) Loss: 0.0000\n",
      "973000 88% (100m 26s) Loss: 0.0030\n",
      "974000 88% (100m 32s) Loss: 0.2018\n",
      "975000 88% (100m 38s) Loss: 0.0087\n",
      "976000 88% (100m 44s) Loss: 0.0301\n",
      "977000 88% (100m 50s) Loss: 2.4493\n",
      "978000 88% (100m 56s) Loss: 0.0000\n",
      "979000 89% (101m 2s) Loss: 0.0020\n",
      "980000 89% (101m 8s) Loss: 3.3033\n",
      "981000 89% (101m 14s) Loss: 0.2752\n",
      "982000 89% (101m 20s) Loss: 0.0324\n",
      "983000 89% (101m 27s) Loss: 2.3102\n",
      "984000 89% (101m 33s) Loss: 0.6816\n",
      "985000 89% (101m 39s) Loss: 0.0016\n",
      "986000 89% (101m 45s) Loss: 0.0004\n",
      "987000 89% (101m 51s) Loss: 0.0039\n",
      "988000 89% (101m 57s) Loss: 0.0026\n",
      "989000 89% (102m 3s) Loss: 0.0209\n",
      "990000 90% (102m 9s) Loss: 0.3508\n",
      "991000 90% (102m 15s) Loss: 2.2610\n",
      "992000 90% (102m 21s) Loss: 0.4785\n",
      "993000 90% (102m 27s) Loss: 2.9030\n",
      "994000 90% (102m 33s) Loss: 0.1080\n",
      "995000 90% (102m 39s) Loss: 0.0000\n",
      "996000 90% (102m 45s) Loss: 1.8267\n",
      "997000 90% (102m 51s) Loss: 0.4614\n",
      "998000 90% (102m 58s) Loss: 1.1517\n",
      "999000 90% (103m 4s) Loss: 0.5476\n",
      "1000000 90% (103m 10s) Loss: 4.4873\n",
      "1001000 91% (103m 16s) Loss: 0.1246\n",
      "1002000 91% (103m 22s) Loss: 2.9915\n",
      "1003000 91% (103m 28s) Loss: 0.0000\n",
      "1004000 91% (103m 34s) Loss: 0.0125\n",
      "1005000 91% (103m 40s) Loss: 0.0048\n",
      "1006000 91% (103m 46s) Loss: 0.0000\n",
      "1007000 91% (103m 52s) Loss: 1.5230\n",
      "1008000 91% (103m 58s) Loss: 0.0212\n",
      "1009000 91% (104m 5s) Loss: 2.9896\n",
      "1010000 91% (104m 11s) Loss: 1.3634\n",
      "1011000 91% (104m 17s) Loss: 0.0075\n",
      "1012000 92% (104m 23s) Loss: 0.0011\n",
      "1013000 92% (104m 29s) Loss: 0.3423\n",
      "1014000 92% (104m 35s) Loss: 0.0292\n",
      "1015000 92% (104m 41s) Loss: 1.9220\n",
      "1016000 92% (104m 47s) Loss: 4.2869\n",
      "1017000 92% (104m 53s) Loss: 0.2212\n",
      "1018000 92% (104m 59s) Loss: 1.3979\n",
      "1019000 92% (105m 5s) Loss: 0.3519\n",
      "1020000 92% (105m 11s) Loss: 0.1546\n",
      "1021000 92% (105m 17s) Loss: 1.9726\n",
      "1022000 92% (105m 23s) Loss: 0.0057\n",
      "1023000 93% (105m 29s) Loss: 0.0004\n",
      "1024000 93% (105m 35s) Loss: 6.4339\n",
      "1025000 93% (105m 41s) Loss: 3.3276\n",
      "1026000 93% (105m 47s) Loss: 2.7480\n",
      "1027000 93% (105m 54s) Loss: 3.2937\n",
      "1028000 93% (106m 0s) Loss: 0.6929\n",
      "1029000 93% (106m 6s) Loss: 0.0283\n",
      "1030000 93% (106m 12s) Loss: 0.0013\n",
      "1031000 93% (106m 18s) Loss: 0.0009\n",
      "1032000 93% (106m 24s) Loss: 4.2486\n",
      "1033000 93% (106m 30s) Loss: 0.5208\n",
      "1034000 94% (106m 36s) Loss: 1.0378\n",
      "1035000 94% (106m 42s) Loss: 0.0020\n",
      "1036000 94% (106m 48s) Loss: 0.0220\n",
      "1037000 94% (106m 54s) Loss: 3.9819\n",
      "1038000 94% (107m 0s) Loss: 0.2948\n",
      "1039000 94% (107m 6s) Loss: 0.0001\n",
      "1040000 94% (107m 12s) Loss: 0.0002\n",
      "1041000 94% (107m 18s) Loss: 3.6263\n",
      "1042000 94% (107m 24s) Loss: 0.3310\n",
      "1043000 94% (107m 31s) Loss: 0.0321\n",
      "1044000 94% (107m 37s) Loss: 0.5657\n",
      "1045000 95% (107m 43s) Loss: 0.0176\n",
      "1046000 95% (107m 49s) Loss: 0.4621\n",
      "1047000 95% (107m 55s) Loss: 0.0390\n",
      "1048000 95% (108m 1s) Loss: 0.7848\n",
      "1049000 95% (108m 7s) Loss: 1.7602\n",
      "1050000 95% (108m 13s) Loss: 0.0178\n",
      "1051000 95% (108m 19s) Loss: 3.0598\n",
      "1052000 95% (108m 25s) Loss: 1.4380\n",
      "1053000 95% (108m 31s) Loss: 0.0040\n",
      "1054000 95% (108m 37s) Loss: 0.4132\n",
      "1055000 95% (108m 44s) Loss: 1.3201\n",
      "1056000 96% (108m 50s) Loss: 0.0000\n",
      "1057000 96% (108m 56s) Loss: 1.9000\n",
      "1058000 96% (109m 2s) Loss: 0.0006\n",
      "1059000 96% (109m 8s) Loss: 0.0102\n",
      "1060000 96% (109m 14s) Loss: 1.2176\n",
      "1061000 96% (109m 20s) Loss: 0.0492\n",
      "1062000 96% (109m 26s) Loss: 0.1174\n",
      "1063000 96% (109m 32s) Loss: 1.2982\n",
      "1064000 96% (109m 38s) Loss: 0.0266\n",
      "1065000 96% (109m 44s) Loss: 0.0004\n",
      "1066000 96% (109m 50s) Loss: 1.1965\n",
      "1067000 97% (109m 56s) Loss: 0.1315\n",
      "1068000 97% (110m 2s) Loss: 2.6074\n",
      "1069000 97% (110m 8s) Loss: 2.3515\n",
      "1070000 97% (110m 15s) Loss: 0.0016\n",
      "1071000 97% (110m 21s) Loss: 0.3066\n",
      "1072000 97% (110m 27s) Loss: 2.5579\n",
      "1073000 97% (110m 33s) Loss: 2.1847\n",
      "1074000 97% (110m 39s) Loss: 0.0761\n",
      "1075000 97% (110m 45s) Loss: 0.9993\n",
      "1076000 97% (110m 51s) Loss: 1.5969\n",
      "1077000 97% (110m 57s) Loss: 2.1794\n",
      "1078000 98% (111m 3s) Loss: 0.0003\n",
      "1079000 98% (111m 9s) Loss: 0.0006\n",
      "1080000 98% (111m 15s) Loss: 0.0342\n",
      "1081000 98% (111m 21s) Loss: 2.8225\n",
      "1082000 98% (111m 27s) Loss: 0.0005\n",
      "1083000 98% (111m 33s) Loss: 0.0002\n",
      "1084000 98% (111m 39s) Loss: 0.4813\n",
      "1085000 98% (111m 45s) Loss: 0.0092\n",
      "1086000 98% (111m 52s) Loss: 0.0002\n",
      "1087000 98% (111m 58s) Loss: 1.0136\n",
      "1088000 98% (112m 4s) Loss: 0.4902\n",
      "1089000 99% (112m 10s) Loss: 0.0105\n",
      "1090000 99% (112m 16s) Loss: 0.2623\n",
      "1091000 99% (112m 22s) Loss: 0.0096\n",
      "1092000 99% (112m 28s) Loss: 0.3097\n",
      "1093000 99% (112m 34s) Loss: 0.1802\n",
      "1094000 99% (112m 40s) Loss: 1.2816\n",
      "1095000 99% (112m 46s) Loss: 0.4910\n",
      "1096000 99% (112m 52s) Loss: 0.0212\n",
      "1097000 99% (112m 58s) Loss: 1.3914\n",
      "1098000 99% (113m 4s) Loss: 0.6546\n",
      "1099000 99% (113m 10s) Loss: 2.7120\n",
      "1100000 100% (113m 16s) Loss: 5.7036\n"
     ]
    }
   ],
   "source": [
    "# Set up the number of iterations, printing and plotting options\n",
    "n_iters = 1100000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "# shuffle indices\n",
    "indices = np.random.permutation(len(sequences))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# run training procedure\n",
    "for iter in range(1, n_iters + 1):\n",
    "    \n",
    "    # Pick index\n",
    "    index = indices[iter % len(sequences)]\n",
    "    \n",
    "    # Run one training step\n",
    "    output, loss = train(sequences[index], targets[index][0].long(), device)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number and loss\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = wordFromOutput(output)\n",
    "        print('%d %d%% (%s) Loss: %.4f' % (iter, iter / n_iters * 100, timeSince(start), loss))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2fcca6bf98>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd8VeXhx/HPc292AiGQMAOEjYjMgAxREFHc2modtXX9pLWuWlt/qL86Sh1117ZarFbrKNVatQo4EEEEZQSRvWdYGayEQOZ9fn/cm5vcrJsANzk3fN+vFy/PPfeck+d48OuT5zzDWGsREZHw4WrqAoiISMMouEVEwoyCW0QkzCi4RUTCjIJbRCTMKLhFRMKMgltEJMwouEVEwoyCW0QkzESE4qLJyck2LS0tFJcWEWmWli5dmmutTanPsSEJ7rS0NDIyMkJxaRGRZskYs72+x6qpREQkzCi4RUTCjIJbRCTMKLhFRMKMgltEJMwouEVEwoyCW0QkzDgquF+YvZGvNuQ0dTFERBzNUcH90tzNLNiU29TFEBFxNEcFtzHg8WjxYhGRujgquF3GoNgWEambo4LbAB6r6BYRqYuzgtuAcltEpG4OC26DVXKLiNTJUcHtMqiNW0QkCEcFtzFGbdwiIkE4KrhdauMWEQnKUcENBnXjFhGpm6OC22VArdwiInVzVHB7R042dSlERJzNWcGNwarGLSJSJ0cFt8ugNm4RkSAcFdzeAThNXQoREWdzWHCjphIRkSCcF9zKbRGROjkquF2aq0REJChHBbd3WtemLoWIiLM5Kri1kIKISHCOCm6MFlIQEQnGUcHtMprXVUQkGEcFt5YuExEJzlHB7dIAHBGRoBwV3EZt3CIiQdUruI0xdxtjVhtjVhljphljYkJRGKNeJSIiQQUNbmNMJ+BOIN1a2x9wA1eHojAGNABHRCSI+jaVRACxxpgIIA7YHZLCuDTkXUQkmKDBba3dBTwN7AD2AIestZ9XPc4YM8kYk2GMycjJyTmmwhi0WLCISDD1aSpJAi4FugEdgXhjzHVVj7PWvmytTbfWpqekpBxbYdSNW0QkqPo0lZwDbLXW5lhrS4D3gVEhKY3RYsEiIsHUJ7h3ACOMMXHGGAOMB9aGpDBGLydFRIKpTxv3IuA94Dtgpe+cl0NRGG+vklBcWUSk+Yioz0HW2oeAh0JcFt/sgEpuEZG6OG/kpKepSyEi4mwOC27VuEVEgnFWcKMVcEREgnFWcBvUkVtEJAhHBbfLaOSkiEgwjgpuLYAjIhKco4Lbu5CColtEpC6OCm7Qy0kRkWAcFdwuLaQgIhKUo4LbaK4SEZGgHBXcWixYRCQ4RwW3dwCOkltEpC7OCm7VuEVEgnJYcKvGLSISjKOC262RkyIiQTkruN2GUnXkFhGpk6OCO9JlKFNwi4jUyVHB7Xa5KC1TcIuI1MVRwR3pNpRqCRwRkTo5KrjdLqMat4hIEI4K7ki3Sy8nRUSCcFRw5x0t4dDREj5dtbepiyIi4liOCu7NOYcBeG3B1iYuiYiIczkquCPc3uIs2rq/iUsiIuJcjgruSLdp6iKIiDieo4LbZRTcIiLBOCq4K/co8ah3iYhIjRwV3JWHuxeXaSCOiEhNHBXclWvcRSUKbhGRmjgquCs3jxSVlTVhSUREnMtRwf3HqwcRFeEtkmrcIiI1c1Rwd09J4OkrBwJQVKrgFhGpiaOCGyDaV+MuLFFTiYhITRwX3HFRbgCOKrhFRGrkuOBuERMJQH5hSROXRETEmRwY3BEA5BeWNnFJREScybHBnafgFhGpkeOCu6WvqSTvqJpKRERqUq/gNsa0Msa8Z4xZZ4xZa4wZGaoCxUS6SYiOIPdwUah+hIhIWIuo53F/BD611l5hjIkC4kJYJtq2iCY7X8EtIlKToMFtjGkJnAncAGCtLQaKQ1molBbRZOcVhvJHiIiErfo0lXQHcoDXjDHLjDGvGGPiQ1moti1jVOMWEalFfYI7AhgCvGStHQwUAJOrHmSMmWSMyTDGZOTk5BxXodq2iCY7rwhrNSe3iEhV9QnuncBOa+0i3+f38AZ5AGvty9badGttekpKynEVKjkhmqMlZRwp1uhJEZGqgga3tXYvkGmM6ePbNR5YE8pCJfj6chcUqS+3iEhV9e1Vcgfwtq9HyRbgxtAVCRKivfOVFKjGLSJSTb2C21r7PZAe4rL4xUd5i/XyvM2c1bstE/u3b6wfLSLieI4bOQkQH+0N7mmLM/n5W0ubuDQiIs7iyOAu0wrvIiK1cmRwj+6Z3NRFEBFxLEcGt9tlAj7PWLGniUoiIuI8jgzuqr5cl93URRARcQzHBve/Jo3wb6/PymPEY7PZe0jzl4iIODa4R3Rvw8UDOwKwalcee/MKmTIjpON+RETCgmODG+CZKwcGfFZbt4iIw4M7KqLm4i3YlEuOZg8UkZOUo4Mb4Odn9Qj4/Pgna/nxK4u48fXFTVQiEZGm5fjg/sGQTgGfp361BfC2e1eWe7iI6St2N1q5RESaSn0nmWoyvdu1qPW71bsPsTHrMLsPHWXx1v3MXZ/DoM6tSE2qWFntzmnLmLUmi7VTJjZGcUVEQs7xwV2XC1+Y799u6ZsKNnP/0YDg/mi5auEi0ryEdXBX1j4xhrzCw0xbvINr/raQM3omc2V6qv97j8fiqjQi8/UFW0luEc1FAzo2RXFFRI5ZWAX3Oae05Yu1NY+i3JB1GKioYc/flMv8Tbn+7w8Xl3LoSAljnpxD9+R4tuQWACi4RSTsOP7lJECHxBgAHrv8tGO+xtJtBxjz5BwAf2gDzFmXTWmZh9zD6l4oIuHBhGJB3vT0dJuRkXHCrnegoJj9R4rpkZKAx2PxWMsXa7P4+VvfHfe175nQmw3Zh/l4+W42P3YBs9dm0aVNHH3btzwBJRcRqR9jzFJrbb0WrAmLGndSfBQ9UhIAcLkMEW4XE/t38H+/6dHzj/naHgsf+5pXlu88yKQ3lzLx+a85WlzGwi37+HRV4GjNAwXFpE2ewdz1gU02y3YcIG3yDPYcOnrMZRERqY+wCO5gItwVt3FVeucGnfvcFxv82z948Rv/9o79R7j65YX8/K3vyM4r5Is1WVz65/l8t+MAAH+cvTHgOm8v2gHA1xtyA/Zn5xUyZ71mNxSREyesXk7Wx+M/OI13MjKP+zrnPT/Pvz38sdn+7cnvrwS8A34WbMolNSmWrm3iiYvyLnB8pNi7Mv2sNVkMSE3k2r8tZHNOASsePpeWMZHHXS4RkbAO7r/9NN0/n0nXNnEUFJX6u/z9cEgqGdv3s33fEQB+MbYHL87dfNw/s3yOlMz9R/nxK4v8+0f3bAPAkZIyCkvKuOUNbxu/8fVAvOKlb/j0rjN5f9kuLhrQgZhIN9n5haQkRGNM4MIRIiJ1CevgntCvnX97zj1j/dsbHz0ftzEYA93umwnAqB7JvDh3MzGRLm4b25NnZm1gZPc2fLtl3wkpy4JN3utMX76HbzdXXLP83e+GrMP8d/kufv3v5WTlFeLxWJ6ZtYEpl57KT0am1XjND5ftok1CFGN6pZyQMopI89As2rjB+9KyvLYd6XbhcpmAmmxctLcp4/LBnRjerTUApR4PD1/cj4ToCGbeOeaElGPNnjy+3phb43fr9uQD3sWQn5nlbVtfsu0As9dm0e/BTzlQUExRaRn//X4X1lp++c73/OTV6pNpfb56L9n5WlRC5GTVbIK7LskJUQzpksQL1wzmwYtO9b/MLCmz3DC6G6seOY9+HVvy49O7AHDvxD7+c28dWzE74cDOrfzbVdfFrI+p87wTZD07q+KF6Krdh/j9jLUcKS7jzYXbeeKTddz1r+95Z0nN7fQlZR4mvbmUq19e2OCfLyLNQ7MP7k/uGsOnvzwTgEsGdiQ2yk2EL3RLPZ6AY12+Gnp8VARf/WYs5/Zrx81ndPN//8LVg/zb1wyv6L0S5T72f41bcgrY6hsQ9OysDby2YBtQ8RIUYH9BMV+uy2L7vgKKSz3+86oqLfNU2ycizU+zD+5TOrQkOSE6YF/fDi0Y3q01v78scCRmeSW6zGPp2iael3+aTnJCNImx3t4gsZFuOrWKBeDG0d0Crlfu/P7tT/g9DJkyi5tez+Csp+ayYuehgO8en7mWX73zPev35tPzgU946L+rTvjPFxFnafbBXZPoCDfv/mwkgyo1fQC0jvcGfEJ04DvbNglRgHdFnkh3RRNJeaD/5doh/n1/uGKAf7vyXOLlgV5+zrG65m8VTSTWWqbO28L7y3axPPMgAP/4djvg7f1SUFR6XD9LRJwprHuVnGg/H9udVnGR/HBoasD+N24azpfrsmkVF8Vjl5/G76avITUplrm/HktBcWnANLKV+2o/dcVA3v9uF+AN928276NH23jeWZLJ818EDuA5Frf/c5l/+w+frvNvW2sZ9ugXpHdN4r1bR/HjVxZyasdE7r/glOP+mSLS9E7KGndtoiPcXD8qrdqLx9SkOH7q67I3qmcyn/7yTKIj3CTFRwWEdrlpt4zgk7vGBFzH5TKc0SuZDomx/msBvHbjMACeqlRTH9+3bb3KO2NlxXD8fQXF/u2563MAyNh+gO37CliwaR8v+16Mlnmqz00z9avNdL9vBqGYt0ZETjwF9wly2aCK6WFH9mjDKR1qn6SqfJQlwLg+bdn2xIVcmd6Z56/yvvw0Bp78oTfIywf2AIzqUbHduXVsrde/8fUl/u03fU0nAJuy8znlwU/5YNlOrLX89sNVfLfjAI9/sg6PhaMlZfW5VRFpYgruE+T5qwez7YkLq+1/8ocD+NM1gwP2Rdeyen1inLeZJT46gs6tvTX5klLrb3NPTaoI6wsqTbJVl1fmb/Vv/2XOZopLPfx17ha25Bbw5sLt/OzNpcRGev9Hsr9SrR3g2c/Xc+97y+v1c0Sk8Si4Q+xHwzpz8cDAxRqMMVyV3pl/3DQ8YP+ZvVK4a3wvHr74VLq28Qb3WX1S+NekEbRtEc1d5/T2H9u1Tbx/++Pbz6hXWT5Y5m1vX5+Vz/hnvgK8LzHjfYOTlu04GHD8C19u4t2MnfW6tog0HgV3E/nDFQM4q3fgUHa3y3D3hN4kxUfRsVUsSx44h1vP6kH/ToksfuAcOrWK9fcft1h/18Re7RL817hhVFqDyxLp64c+a00WyzMP8tLczZRU6RNurfXXyK217D6o6WtFmop6lThYSovoavuiI7y1Y4PhvVtHsnpXHjGRFW3mt5zZncNFpdw0uhtREYZznp1X7RpV7TnkHT7/0fLd/qXfKvdSKS3z8NqCbTw6cy0LJp/N6Ce+BGDmnWPo11ELTog0NgV3mLl7gre55AdDOhET6aZDorfW3aV1HDv2HyEpLpKnrxxY7byoCJd/1GVDvbd0J9N9PVgWVppAa0vuYQ4eKWbh1v38akLv2k4XkRNMwR1mEmMjefiSU6vtnzZpBPM35hAXVfMj3fD789lfUMyBI8Us2rKf+z9YWeNxNXlx7mZ27PdOj3vPvyteVloL1/qmtr19XE+e+2IDL83dXONLWhE5cdTG3Ux0ahXLVcO61HlMa98ScNeeXnFc1RekNSkP7aoqt4NnHjjCS775zgvr6FaoWQ1Fjp+Cu5lb/MB45v56bI3fxUa6A16QTr/D2zulZUz9fhH71bsVte/znqtoS887WsJ976/wdyVcsfMgC7fsY9mOAwx/dDYf+nq3iMixqXdwG2PcxphlxpjpoSyQnFhtW8SQlhxfbf/8/x3HgslnB+w7tWNL7hzfi3//fFSDf05ppRGZs9ZmMW1xJu9m7CSvsIRL/ryAq19eyLZ93hkNZ6/TGpwix6MhNe67gLWhKog0rtSkOFrHeyfPmnX3mcz7zTiMMfxqQm/6tG/Bh7eN9h/76OX9ATi30opDLeqolT/wQcUMhZVr4jG+HjEHjwQO9Fm7J4+fvLqoziYWEalQr+A2xqQCFwKvhLY40hR6tWtBlzaBc66c1inRv331sC48eFE/XrhmsH+xiXNOaUd9lHc1BO+AHoCvN+YGjNL83cdr+HpjLku3HzjmexA5mdS3V8nzwL1Ai2AHSvPgdhl+lJ7KJQM74XYZbvItKDG8W2veXrSDkjIPmx+7gD2HjmItjHlyTtBrrt2T598eMmUW4J2XpXwOltzDRdXOeXPhds7qlVLtfywiJ7OgwW2MuQjIttYuNcaMreO4ScAkgC5d6u7dIOHhySuq9wcf2jUJgCuGpuJ2Gf/siGN6JfP1xlySE6LIPeytTXduHUvm/rpHWFpbsdDyV+tzmNi/PaOf+JKLB3akVWwUz32xgS6t45h377gTeWsiYa0+TSWjgUuMMduAfwFnG2PeqnqQtfZla226tTY9JUWrkjdXqUlxbHviQsb2CZx6tnwK2yevGMCy307gxtFpzLr7rAZd++MVu9mcXUDu4WJeW7CN577wrs2Zk1/EwSPF3DFtGQeqTIQlcjIKGtzW2vustanW2jTgauBLa+11IS+ZhJU7zu5FYmwkgzsnkRQfxUMXnxowFH/dlIn+7fKXnVWVlFkueOHravuPlpTxzpJMPl6+m6nztrD74FGGTJnFhqz8E38jImFA/bjlhBjaNYnlD51Lkq+nSrl//s/p3Dg6LSDEL6kyW2J9lC+o/M9F23ls5lr2FxTzz0U7KPNY9h7SoB45uTQouK21c621F4WqMNL8jOqZzEMXe4fod/HNMV4+URbAgNSK3itTLqu5Jg7wryWZAOQVljJ9hXfelE9W7eHxmWsZ8fhs5qxX33A5eajGLY3mnZ+N4IVrBhMV4WLp/53D4vvH8+ZNp/u/v3Z4w15qZ+UV8W6GN9BvfG0Jew8VctXUb1m3N48bX1vMn2Yf/7qeIk6kSaak0XRIjOWSgd7ZDNskVJ+ytvwFZ4voCN7/xSgmPBd8Stq8woqV7Ec8PhuAic9728nnrM+hX8eWjK9nn3ORcKEatzS5+y/oy3UjvLXtj24fzaxfnUWvdhVDBqbdMgKADokxRLkb9lf25n9k+LettTw+cy3bfUPvRcKVatzS5Cad2cO/PSC1VbXvh3drzXUjunDDqG4UlpRx0Z/mN+j6q3Yd4oEPV3H1sM5MnbeFWWuy+LKWibdEwoGCWxzrjJ7JDOrcCrfL8PvLTgNgm693SZv4KJ6+ciA3vr6Evu1bsG5v7V0Dy4N+eaZ3Tc1tqnFLmFNTiTjWW/9zOr8+r0/Avrgo39JtBsb1bctfrxvKazcO86/Ac+FpHYJe12MhY9t+Zq7cw4/++i3LdhwgbfIMPlimhZElPCi4JawkxkUC3gE/ABP7t6dDYiwxkd6/yu1axnDT6G5Br/Pq/K384u3vWLxtP5e/+A0Ad7/jnT/cWou1tq7TRZqUglvCSnSEm21PXMj1VVazL19YuUNiDN2Svf3Frz29C6d08C5mPCwtyX9sp1axfLJqb43Xzz1cRLf7ZvLIx2vweCxpk2c0aJk3kcag4JZm4bJBnfjTNYO56Yxu/nU3S8s8/ppzl9YVi0n8cGhqrddJ//0XALz+zTYu+Yu3bfyfi3aEqtgix0TBLc2CMYaLB3bE7TJcOKADV6V35tfn9eHBi/rRPTmeRy6tWGC5fBrZYFbtqpiGdsXOgzz431Vk1rL+pkhjUnBLsxMT6eYPVwygbYsYRvVM5stfjyUhOsK/+IOnUvt1+Tqbwfzpy0288e12xjw5h90Hj/pnKTxQUMzmnMMn/iZE6qDglpPGEz88jTvO7snp3drwg8GdAOhfaaWfqT8ZWuu5s9Zk+bdvfWspg32zE97w+hLGP/MVZR69zJTGo+CWk0ZyQjT3nNsHt8vw7FWD2PbEhQHfl6+pmRAdwfrfT6zpEgAs33kIgJteX8LqXd7tHvfPpLTME6KSiwRScMtJ70rfy0pjDCsfPpdF948PmMGwNjsPHPUvuAxw4EgJ1lpKyzy8m5HJh8t2hazMcnIzoeivmp6ebjMyMoIfKOIAHo/FUjHJVbm0yTMA2PzYBfR/6DMslsKSumvVN43uxt8XbPV/rlqrF6mNMWaptTa9Pseqxi0nPZfLVAvtytwuw6IHxrPmkYm88tO6/7uqHNoAh46WMGPFHkY9PpviUjWlyImhuUpE6qFljHfE5ri+bYMcGWjw7z4nPiqC/KJSJr2ZwfLMg9w0uht3jO8VimLKSUI1bpFaLLxvPAsmnx2wr3LN/O3/Ob3qKdV4LOQXeecMn7s+hwNHSnhmlncRZI9HQ+vl2Ci4RWrRPjGGTq1iq+2/Z0Jv3rx5OKN7Jvv3zbxzTIOuvX1fAZe9uICrpi487nLKyUdNJSINVFMzR5sEb++S60d2ZdriTIrLPLSJj2Kfb6BOVWt257HC161w/d581mflM6BTImnJ8TUeL1KZepWIHIf/fr+L1KQ4hnZNYs+ho7RtEcOeQ0c54w9z6JYcz4jubZi2eAfJCdHkHi6q81rJCdFk/N85bM45TGykm4411Pal+VKvEpFGcumgTgzt6p15sENiLG6XoVOrWH52Znde/slQ//zgU38yhI6JMQD0bd+ixmvlHi5i+ordjH/mK0Y98SVfb8xpnJuQsKMat0iIlZR5iHS7+MOn63hp7mZ+fW5vZq3N9q/IU5frR3blkUv7N0Ippampxi3iIJG+BY7LFzouLvXw6vXp/PW6IVw6qGOd5/7j2+28m5HJttwCJjz7FSt2Bg97af4U3CKNZGyfFADO6JVCckI0E/t34IELTwl63r3vrWD6it1szD7MJX9eQHZ+YaiLKg6nphKRRmStxZjAUZqjHp/N7kOF/PW6ofz8raUAtGsZTVZe7S8zrxiaSrfkeG4+oxuHi0pJTogOabkl9NRUIuJQVUMb4PIh3ilmT+/WmnVTJvLajcMY3Dmp2nGVvbd0J099tp4xT84h/fdf8M2mXMA7qEeaPwW3SBO7Z0IfvvvtBJLio4iJdDOuT1umXFbxQvKiAbWvXJ+T762VL8s8yOrdh+h+/0y+2ZzLwi37tGp9M6YBOCJNzOUyAdPDgnfx45l3jmH7vgLG9W3L9BV76rzGuxmZ/lV55m/M5cW5mwG4fHDt62tK+FKNW8Sh+nVsyfmndSAmsvrc4H3aBfYF377vCK/M985MWB7aAFtzC5i3IYe0yTPIygt8qbly5yGe882bIuFFNW6RMDC6ZxsWbNrH1scvoLwZu8f9M4OeN+mNDOKivMG/dk8e7VrG+L+7/MUFlHost43rSVSE6nDhRMEtEgZevX4YBUWlGGNw+95vDkhN9M93UpuN2RULGd/w2hLOO7UdT14xkMTYSEp9/wfILyyhjXqlhBX9b1YkDMREuquF67s/G0lspJvUpFj+et0QAKZcemqd1/lsdRYDH/k8oPdJXmEp+wuK+Xz13hNfcAkJ1bhFwlRMpJvVj5yHMd5uhtueuBBrLX+Zs5m9eXUP0vm+0gjMvKMl3P7P71i9O49Vj5xHQrRiwelU4xYJYy6XCegbboxh9j1n8fndZ7Ly4XO5aEAHeqTEM+2WEQHnZe4/4l8U4sCRYlbvzgPgcGEpZz01h1fnBy7BJs6ikZMiJ4HDRaX0f+izoMf9akJvnvX1NNFCx41LIydFJEB8VPUuhTV5tkr3wAc+WMlHy3dXO+5wUSmFJWUnpGzScGrMEjkJGGP4z60j6ZwUR6TbxVlPzSGvsLTOc37698XM25DD24t2cMnAjgHzrPR/6DPS2sQx9zfjGqP4UoVq3CIniaFdW9O2ZQxJ8VEs/e0ExvRK5jfn9an1+HkbKhZy2F9QTK8HPuHDZbv8+7btO0JJmSekZZaaBQ1uY0xnY8wcY8xaY8xqY8xdjVEwEQmdSLeLN28+ncsGd/Lvm3bLiFp7lAyZMotSj2Xy+ysC9t///sqQllNqVp8adylwj7X2FGAEcJsxpl9oiyUijaFTq1i+++0Etj5+ASN7tOHqYZ3rPL6wxMOcddn+z/9eupOtuQWhLqZUETS4rbV7rLXf+bbzgbVAp7rPEpFw0To+yt92XV4Dv2VMNwBc1Weh5cbXlwR8Hvf0XL7Z7J1WNhS91KS6BnUHNMakAfOA/tbavCrfTQImAXTp0mXo9u3bT1wpRaTJ3DFtGR/X0LOkqsUPjOdHf/2WW8f24KphXRqhZM1LSLoDGmMSgP8Av6wa2gDW2pettenW2vSUlJT6l1ZEHG3y+X1p75uc6pQOLWs9buGW/Wzbd4T//c9K3s3I5Na3lqoGHiL1qnEbYyKB6cBn1tpngx2vATgizYvHYykoLqVFTCT/zshk9e48Xv9mW9Dz3pk0gtO7twl9AZuBhtS4g/bjNt7Gr1eBtfUJbRFpflwuQ4uYSACuTO/MlcDwbq3JLywhOSGam/9Rc0Vtxso9DOmaxBvfbufqYZ2J1zwoJ0R9/i2OBn4CrDTGfO/bd7+1NvhkwCLSbF1wmndJNWstgzq3Iie/iFM6tOSLtVn+Y974djtvfOt935WdV8h9F1Rf1f7z1Xt58rP1fHLXGCLdGlpSH0GD21o7H6jh3bKIiHdU5oe3jfZ/Tps8o8bjps7bwtl92zKkaxKRbhcejyW/qJQ7/7WMwhIPB4+UkNJC84LXh35vEZETavmD5xIT5aKw2MPA330e8N1VLy+s9bysvEIFdz3p9xIROaES4yKJjnCTGBfp39c9JT7oeRf9aT4LNuXi8VhW7657ZZ+TnaZ1FZGQySssIcrtwu0yHC0pY8DDnwc9Z2BqIst3HuKj20czILVVI5TSGTStq4g4QsuYSGIi3US6XbSMieTre8dx3Yi6B+cs962jOfWrLQH7y3zLrW3KPkzpST65lWrcItLoDh0tYeAjwWvfvzmvD099tp5OrWIpKi2jqMRDflEpN4xKo0NiDL3bt2Bcn7YB5xwuKiUu0o2rpvH6DtaQGreCW0SaREmZh90Hj3LWU3MBePCifvxu+pp6nRsb6eaobyGHf95yOiO6tcHlMhQUlXLqQ5/xi7E9uHdi31AVPSTUVCIijhfpdtG1TTzbnriQbU9cyI+GdebM3t7pMs7u27bOc49WWn3n2r8t8o/inDrP27zybkZmaArtEOoOKCKOkBAdwRs3DSfb1y3wy3XZ1UZkDurciu8zD1Y7d2N2PgAvzN4IQFFp824DV41bRBylbcsYjDF6sMEdAAAIHklEQVSMP6Ud/71tNP+5dRRTLuvP4vvHc+vYHjWeM21xJjn5Rf7PlVfmmbUmi7TJM9h18ChHikvJzi8M+T2EmmrcIuJYAzt7uwMO7ZoEwHmntue1G4bx8fLd5BeVMmtNxfD6YY9+4d8uLKkI7n/7mk2WZx5k6rwtLM88GPYr2KvGLSJhZVzftjx71SDaBhll+ff5WzlSXMoKX/fCwpIylldpZvl89V52HzwasrKGioJbRMLSvefV3Wvkd9PX0O/Bz9ib520aeXvRDv93S7cfYNaaLCa9uZTLX1wAwNHiMuauz67xWrXJPVzEs7M2+PuYNxY1lYhIWEqMi2TdlInsOVTIc7M28FGQVXqWbj/g3/7hS9/4t7PyitiaW8Btb3/Hmj15fDP5bDq2iq1XGR74YCWfrc5iZPc2jOzRePOOK7hFJGzFRLrplhzP01cO5PLBnYiOdDGqRzK3vf0dM1buqfd1fvr3RWTu9zaZHDxSUu/gPnikBGj8tTbVVCIiYS8qwsW4vm0Z1SMZgKevHMjHt5/BRQO8c4a/+OMhdZ5fHtoABcWltR7n8TWJZOcVMmX6Gn9/8uJGHoKvGreINDuxUW5OS03kz9cO4c/Xevd1bh3rD+h3Jo2odYrZz1fv5Ys1WazafYi/XDuEBz5cxd3n9GLtnnzumLaMCwd0IDuvkCXbKppe8gtrD/tQUHCLyEnh07vOZPa6bHqmJNCvY8Wix1Mu689vP1zl//y3r7f6t5/5fAMzVuxh/+FioiK8DRQzVuwhrU1cwLX3FxSHuPSBNFeJiJyUcvKLiHK7aBETQff7g6/E2LZFNNm+QT7tW8b4e6sAjOuTwu1n92RgaisijnH5NU0yJSLSABuy8vlyXTYtYyK5/4OVQY9vGRNBnq95pEVMhL+p5LROiXx8xxnHVIYTusq7iEhz17tdC3q3awHAtad3IXP/EcY8OafW4/MqtWlfOqgjby309hFfuatxVu5RrxIRkSo6t47jN+f1AeBXE3oD0Ld9i2qzFt4wKs0f+OUao2ugatwiIjW4bVxPbj2rBy6X4YqhqXRI9E5+9eiMNf4XmA9fciqLt+73n/PhbaMxJvQLOCi4RURqUb6KTuUBOZPPP4WYSDcju3tHSg7v1prlD54bsDhyqCm4RUQawO0y3HNun4B9jRnaoDZuEZGwo+AWEQkzCm4RkTCj4BYRCTMKbhGRMKPgFhEJMwpuEZEwo+AWEQkzIZkd0BiTA2w/xtOTgdwTWBwnac73Bs37/nRv4Stc7q+rtTalPgeGJLiPhzEmo75TG4ab5nxv0LzvT/cWvprj/ampREQkzCi4RUTCjBOD++WmLkAINed7g+Z9f7q38NXs7s9xbdwiIlI3J9a4RUSkDo4JbmPMRGPMemPMJmPM5KYuT0MZYzobY+YYY9YaY1YbY+7y7W9tjJlljNno+2eSb78xxrzgu98VxpghTXsH9WOMcRtjlhljpvs+dzPGLPLd3zvGmCjf/mjf502+79OastzBGGNaGWPeM8as8z3Dkc3p2Rlj7vb9vVxljJlmjIkJ12dnjPm7MSbbGLOq0r4GPytjzPW+4zcaY65vins5Vo4IbmOMG/gLcD7QD7jGGNOvaUvVYKXAPdbaU4ARwG2+e5gMzLbW9gJm+z6D9157+f5MAl5q/CIfk7uAtZU+/wF4znd/B4CbfftvBg5Ya3sCz/mOc7I/Ap9aa/sCA/HeY7N4dsaYTsCdQLq1tj/gBq4mfJ/d68DEKvsa9KyMMa2Bh4DTgeHAQ+VhHxastU3+BxgJfFbp833AfU1druO8p/8CE4D1QAffvg7Aet/2VOCaSsf7j3PqHyAV738UZwPTAYN3YENE1ecIfAaM9G1H+I4zTX0PtdxXS2Br1fI1l2cHdAIygda+ZzEdOC+cnx2QBqw61mcFXANMrbQ/4Din/3FEjZuKv1jldvr2hSXfr5aDgUVAO2vtHgDfP8uXiQ7He34euBfw+D63AQ5aa0t9nyvfg//+fN8f8h3vRN2BHOA1XzPQK8aYeJrJs7PW7gKeBnYAe/A+i6U0j2dXrqHPKqyeYVVOCe6alkUOy+4uxpgE4D/AL621eXUdWsM+x96zMeYiINtau7Ty7hoOtfX4zmkigCHAS9bawUABFb9q1ySc7g1fE8ClQDegIxCPtwmhqnB8dsHUdi9hfY9OCe6dQOdKn1OB3U1UlmNmjInEG9pvW2vf9+3OMsZ08H3fAcj27Q+3ex4NXGKM2Qb8C29zyfNAK2NM+aLTle/Bf3++7xOB/Y1Z4AbYCey01i7yfX4Pb5A3l2d3DrDVWptjrS0B3gdG0TyeXbmGPqtwe4YBnBLcS4BevrfcUXhfnHzUxGVqEGOMAV4F1lprn6301UdA+Rvr6/G2fZfv/6nvrfcI4FD5r3pOZK29z1qbaq1Nw/t8vrTW/hiYA1zhO6zq/ZXf9xW+4x1Zo7HW7gUyjTHlS3ePB9bQTJ4d3iaSEcaYON/f0/L7C/tnV0lDn9VnwLnGmCTfbyTn+vaFh6ZuZK/0cuACYAOwGXigqctzDOU/A++vWiuA731/LsDbNjgb2Oj7Z2vf8QZvT5rNwEq8b/yb/D7qea9jgem+7e7AYmAT8G8g2rc/xvd5k+/77k1d7iD3NAjI8D2/D4Gk5vTsgEeAdcAq4E0gOlyfHTANb1t9Cd6a883H8qyAm3z3uAm4sanvqyF/NHJSRCTMOKWpRERE6knBLSISZhTcIiJhRsEtIhJmFNwiImFGwS0iEmYU3CIiYUbBLSISZv4f2Cwa7TyZ0XsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training loss\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that __training loss is decreasing__ over the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Kernel Titles from the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our trained model to generate new kernel titles! All we need to do is to write a simple sampling procedure:\n",
    "* Introduce the maximum number of words in the title (10 for example);\n",
    "* Pass zero tensors to the model as the initial word and hidden state;\n",
    "* Repeat following steps until the end of the title symbol is sampled or the number of maximum words in title exceeded:\n",
    "\n",
    "    * Use the probabilities from the output of the model to get the next word for a sequence;\n",
    "    * Pass sampled word as a next input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample title from the trained model\n",
    "def sample():   \n",
    "    num_words = 10\n",
    "    \n",
    "    # Initialize input step and hidden state\n",
    "    input = torch.zeros(1, 1, vocab_size)\n",
    "    hidden = (torch.zeros(1, 1, n_hidden).to(device), torch.zeros(1, 1, n_hidden).to(device))\n",
    "    \n",
    "    i = 0\n",
    "    output_word = None\n",
    "    sentence = []\n",
    "    # Sample words from the model\n",
    "    while output_word != '.' and i < num_words:\n",
    "          \n",
    "        input = input.to(device)\n",
    "        output, next_hidden = rnn(input[0], hidden)\n",
    "        \n",
    "        y = output.clone()\n",
    "        y = y.to(device)\n",
    "        # use probabilities from the output to choose the next word\n",
    "        idx = np.random.choice(range(vocab_size), p = f.softmax(y, dim=1).detach().cpu().numpy().ravel())\n",
    "        \n",
    "        output_word = [k for (k, v) in vocab.items() if v == idx][0]\n",
    "        sentence.append(output_word)\n",
    "         \n",
    "        hidden = next_hidden\n",
    "        input = wordToTensor(output_word)\n",
    "        i = i+1\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 15 titles from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simply entity graph ezgi .\n",
      "\n",
      "\n",
      "missingno step only using .\n",
      "\n",
      "\n",
      "onlylogisticregression digitrecog d r .\n",
      "\n",
      "\n",
      " porcupine .\n",
      "\n",
      "\n",
      "dinner lesson web screening .\n",
      "\n",
      "\n",
      "south uniq .\n",
      "\n",
      "\n",
      "tyler pokemongo on r am data data lb .\n",
      "\n",
      "\n",
      "caretrun crew notebo only .\n",
      "\n",
      "\n",
      "pics clearer | colorado using only .\n",
      "\n",
      "\n",
      "elasticnet playground .\n",
      "\n",
      "\n",
      "gtlv excavating fine blackjack using .\n",
      "\n",
      "\n",
      "ortools estonia d analyzer .\n",
      "\n",
      "\n",
      "combos maritime .\n",
      "\n",
      "\n",
      "lifespan quit d using .\n",
      "\n",
      "\n",
      "hoxosh aufgabe unfinished behaviour .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample 15 titles and print\n",
    "for i in range(15):\n",
    "    sampled_title = sample()\n",
    "    title = ' '.join(sampled_title)\n",
    "    print(title)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for later\n",
    "checkpoint = {'input_size': vocab_size,\n",
    "          'output_size': vocab_size,\n",
    "          'hidden_size': n_hidden,\n",
    "          'state_dict': rnn.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'rnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel:\n",
    "* I loaded and did a simple preprocessing of real text data.\n",
    "* I created a sequence word based model, which can be used to generate new kernel titles.\n",
    "\n",
    "You can see that the model doesn't generate something that makes sence, but there are still some funny results like these:\n",
    "* \"wealth bowl datamining\"\n",
    "* \"supplement approved databases\"\n",
    "* \"plane ignore population competition\"\n",
    "* \"projecting superfood prescribing survey\"\n",
    "* \"mediation cta aluminum kernals reviews\"\n",
    "\n",
    "This happens when models crush into a __real life data__. They contain abbreviations, nicknames, words in different languages, misspelled words and a lot more. Of course, these results can be improved by __better data preprocessing__. I decsribed actions to improve the results in `\"Further Improvement\"` section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvement\n",
    "Though I managed to get some interesting results, there is a lot what should be done here:\n",
    "* __Better data cleaning__: a lot of titles should be removed from the analysis as they are not in English or they're just can't be used (for example 'kernel123').\n",
    "* __Auto-correction of misspelled words__: titles can be preprocessed with automatic correction of misspelled words (for example, consider [PySpell package](https://facelessuser.github.io/pyspelling/)). I could add this to this kernel, but this would run too long for Kaggle. But this is still an option since data preprocessing happens just one time before training and preprocessed data could be saved for later.\n",
    "* __Hyperparameter tuning__: I suppose that learning rate and sequence length can be tuned to achieve even better results.\n",
    "* __Use [word embeddings](https://hackernoon.com/word-embeddings-in-nlp-and-its-applications-fab15eaf7430) instead of one-hot encoding__ for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "I also want to share some great resources on RNNs and LSTM:\n",
    "1. [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/): Article about RNNs and their use cases.\n",
    "2. [Long Short-Term Memory: From Zero to Hero with PyTorch](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/): PyTorch implementation of LSTMs.\n",
    "3. [LSTMs for Time Series in PyTorch](https://www.jessicayung.com/lstms-for-time-series-in-pytorch/): another LSTM implementation.\n",
    "4. [Exercise: Sampling from an RNN](https://pytorch-nlp-tutorial-ny2018.readthedocs.io/en/latest/day2/sampling.html): examples of sampling from RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
